# âš™ï¸ 03. í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

> ê°œë°œ í™˜ê²½ êµ¬ì¶•ë¶€í„° ê° ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜ê¹Œì§€ ë‹¨ê³„ë³„ ê°€ì´ë“œ

---

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìµœì†Œ ì‚¬ì–‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ìµœì†Œ ì‚¬ì–‘ (ê¶Œì¥ ì‚¬ì–‘)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CPU:    4ì½”ì–´ ì´ìƒ (8ì½”ì–´ ì´ìƒ)                               â”‚
â”‚  RAM:    16GB (32GB)                                           â”‚
â”‚  GPU:    NVIDIA RTX 3060 / VRAM 6GB (RTX 4060Ti / 8GB)        â”‚
â”‚  ì €ì¥ì†Œ: SSD 100GB (200GB)                                     â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸ ì´ í”„ë¡œì íŠ¸ëŠ” 4060Ti + RAM 32GB ê¸°ì¤€ìœ¼ë¡œ ì„¤ê³„ë¨           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ë²„ì „ | í™•ì¸ ëª…ë ¹ì–´ |
|------|------|------------|
| Docker | 27.4+ | `docker --version` |
| Docker Compose | 2.30+ | `docker compose version` |
| Go | 1.24+ | `go version` |
| Python | 3.12+ | `python --version` |
| NVIDIA Driver | 550+ | `nvidia-smi` |
| CUDA | 12.4+ | `nvcc --version` |
| Git | 2.40+ | `git --version` |

---

## ğŸªŸ Windows (WSL2) í™˜ê²½ ì„¤ì •

### 1. WSL2 ì„¤ì¹˜ ë° ì„¤ì •

```powershell
# 1. WSL ì„¤ì¹˜ (PowerShell ê´€ë¦¬ì ê¶Œí•œ)
wsl --install

# 2. Ubuntu 22.04 ì„¤ì¹˜
wsl --install -d Ubuntu-22.04

# 3. WSL ë²„ì „ í™•ì¸
wsl -l -v
# Ubuntu-22.04ê°€ VERSION 2ë¡œ í‘œì‹œë˜ì–´ì•¼ í•¨

# 4. ê¸°ë³¸ ë°°í¬íŒ ì„¤ì •
wsl --set-default Ubuntu-22.04
```

### 2. NVIDIA GPU ë“œë¼ì´ë²„ (Windows)

```powershell
# 1. Windowsì—ì„œ NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
# https://www.nvidia.com/download/index.aspx ì—ì„œ ë‹¤ìš´ë¡œë“œ
# ë˜ëŠ” GeForce Experience ì‚¬ìš©

# 2. ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi
# CUDA Version: 12.x í‘œì‹œ í™•ì¸
```

### 3. Docker Desktop ì„¤ì •

```
1. Docker Desktop ì„¤ì¹˜ (https://www.docker.com/products/docker-desktop/)

2. Settings > Resources > WSL Integration
   - "Enable integration with my default WSL distro" í™œì„±í™”
   - Ubuntu-22.04 í™œì„±í™”

3. Settings > Resources > Advanced
   - Memory: 16GB ì´ìƒ í• ë‹¹
   - CPUs: 4ê°œ ì´ìƒ í• ë‹¹

4. WSLì—ì„œ Docker í™•ì¸
   $ docker run hello-world
```

### 4. NVIDIA Container Toolkit (WSL2 ë‚´ë¶€)

```bash
# WSL2 Ubuntuì—ì„œ ì‹¤í–‰

# 1. NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# 2. Docker ì¬ì‹œì‘
sudo systemctl restart docker

# 3. GPU í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

---

## ğŸ§ Linux í™˜ê²½ ì„¤ì •

### 1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜

```bash
# Ubuntu 22.04 ê¸°ì¤€

# 1. ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo apt update
sudo apt install -y nvidia-driver-535

# 2. ì¬ë¶€íŒ…
sudo reboot

# 3. í™•ì¸
nvidia-smi
```

### 2. Docker ì„¤ì¹˜

```bash
# 1. ì´ì „ ë²„ì „ ì œê±°
sudo apt remove docker docker-engine docker.io containerd runc

# 2. Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# 3. ì‚¬ìš©ì ê·¸ë£¹ ì¶”ê°€ (sudo ì—†ì´ docker ëª…ë ¹ ì‹¤í–‰)
sudo usermod -aG docker $USER
newgrp docker

# 4. í™•ì¸
docker run hello-world
```

### 3. NVIDIA Container Toolkit

```bash
# (ìœ„ WSL2 ì„¹ì…˜ê³¼ ë™ì¼)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

---

## ğŸ¦™ Ollama ì„¤ì¹˜

### Linux / WSL2

```bash
# 1. Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# 2. ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve &
# ë˜ëŠ” systemdë¡œ ê´€ë¦¬
# sudo systemctl enable ollama
# sudo systemctl start ollama

# 3. Mistral ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull mistral:7b-instruct-v0.2-q4_K_S

# 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸
ollama run mistral:7b-instruct-v0.2-q4_K_S "Hello, world!"

# 5. API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "mistral:7b-instruct-v0.2-q4_K_S",
  "prompt": "Hello, world!",
  "stream": false
}'
```

### macOS

```bash
# Homebrewë¡œ ì„¤ì¹˜
brew install ollama

# ë‚˜ë¨¸ì§€ëŠ” Linuxì™€ ë™ì¼
ollama serve &
ollama pull mistral:7b-instruct-v0.2-q4_K_S
```

### Ollama í™˜ê²½ ë³€ìˆ˜

```bash
# ~/.bashrc ë˜ëŠ” ~/.zshrcì— ì¶”ê°€

# Ollama ì„¤ì •
export OLLAMA_HOST=0.0.0.0:11434     # ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©
export OLLAMA_MODELS=~/.ollama/models # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
export OLLAMA_NUM_PARALLEL=2          # ë™ì‹œ ìš”ì²­ ìˆ˜
export OLLAMA_MAX_LOADED_MODELS=1     # ë™ì‹œ ë¡œë“œ ëª¨ë¸ ìˆ˜ (VRAM ì ˆì•½)
```

---

## ğŸ Python í™˜ê²½ ì„¤ì •

### 1. pyenvë¡œ Python ë²„ì „ ê´€ë¦¬ (ê¶Œì¥)

```bash
# 1. pyenv ì„¤ì¹˜
curl https://pyenv.run | bash

# 2. í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€ (~/.bashrc)
export PYENV_ROOT="$HOME/.pyenv"
export PATH="$PYENV_ROOT/bin:$PATH"
eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"

# 3. ì¬ë¡œê·¸ì¸ í›„ Python 3.12 ì„¤ì¹˜
pyenv install 3.12.8
pyenv global 3.12.8

# 4. í™•ì¸
python --version
# Python 3.12.8
```

### 2. ê°€ìƒ í™˜ê²½ ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ
cd content-arena/ml-service

# venv ìƒì„±
python -m venv venv

# í™œì„±í™”
source venv/bin/activate  # Linux/macOS
# ë˜ëŠ”
.\venv\Scripts\activate   # Windows

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip
```

### 3. PyTorch + CUDA ì„¤ì¹˜

```bash
# CUDA 12.4 ë²„ì „ (4060Ti ìµœì )
pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

### 4. ML íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# requirements.txt
pip install -r requirements.txt
```

**requirements.txt:**

```txt
# Core ML (ë³´ìˆ˜ì  ë²„ì „ - ì•ˆì •ì„± ìš°ì„ )
torch==2.5.1
transformers==4.48.3
accelerate==1.5.2
datasets==3.2.0
safetensors==0.4.5

# QLoRA
peft==0.14.0
bitsandbytes==0.49.1

# API
fastapi==0.115.6
uvicorn[standard]==0.32.1
httpx==0.28.1
pydantic==2.10.3

# MLflow
mlflow==2.22.4

# Redis
redis==5.2.1

# Utilities
python-dotenv==1.0.1
numpy==1.26.4
pandas==2.2.0
scikit-learn==1.4.0
tqdm==4.67.0

# Testing
pytest==8.0.0
pytest-asyncio==0.24.0
```

### 5. bitsandbytes ì„¤ì¹˜ (Windows ì£¼ì˜)

```bash
# Linux/WSL2 - ìµœì‹  ë²„ì „ì€ Windowsë„ ê³µì‹ ì§€ì›
pip install bitsandbytes==0.49.1

# ì„¤ì¹˜ í™•ì¸
python -c "import bitsandbytes as bnb; print(f'bitsandbytes: {bnb.__version__}')"

# âš ï¸ Intel XPU ì‚¬ìš© ì‹œ PyTorch 2.6.0+ í•„ìˆ˜
```

---

## ğŸ¹ Go í™˜ê²½ ì„¤ì •

### 1. Go ì„¤ì¹˜

```bash
# Linux/WSL2 - Go 1.24 (ê¶Œì¥)
# âš ï¸ Go 1.23.xëŠ” EOL (2025ë…„ 8ì›”) - ë³´ì•ˆ íŒ¨ì¹˜ ì—†ìŒ
wget https://go.dev/dl/go1.24.0.linux-amd64.tar.gz
sudo rm -rf /usr/local/go
sudo tar -C /usr/local -xzf go1.24.0.linux-amd64.tar.gz

# PATH ì¶”ê°€ (~/.bashrc)
export PATH=$PATH:/usr/local/go/bin
export GOPATH=$HOME/go
export PATH=$PATH:$GOPATH/bin

# í™•ì¸
go version
# go version go1.24 linux/amd64
```

### 2. í”„ë¡œì íŠ¸ ì´ˆê¸°í™”

```bash
cd content-arena/api-service

# ëª¨ë“ˆ ì´ˆê¸°í™”
go mod init content-arena/api-service

# ì˜ì¡´ì„± ì„¤ì¹˜ (2026ë…„ 1ì›” ê¸°ì¤€ ìµœì‹  ì•ˆì • ë²„ì „)
go get github.com/gin-gonic/gin@v1.10.0
go get gorm.io/gorm@v1.25.12
go get gorm.io/driver/postgres@v1.5.9
go get github.com/redis/go-redis/v9@v9.7.0
go get github.com/prometheus/client_golang@v1.20.0
go get github.com/spf13/viper@v1.19.0
go get go.uber.org/zap@v1.27.0

# ì˜ì¡´ì„± ì •ë¦¬
go mod tidy
```

---

## ğŸ³ Docker ì¸í”„ë¼ ì‹¤í–‰

### 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±

```bash
mkdir -p content-arena/{api-service,ml-service,infra}
cd content-arena
```

### 2. docker-compose.yml ìƒì„±

```yaml
# infra/docker-compose.yml
version: "3.9"

services:
  postgres:
    image: postgres:16-alpine
    container_name: arena-postgres
    environment:
      POSTGRES_DB: content_arena
      POSTGRES_USER: arena
      POSTGRES_PASSWORD: arena_secret_123
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U arena -d content_arena"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: arena-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    container_name: arena-mlflow
    ports:
      - "5000:5000"
    environment:
      MLFLOW_TRACKING_URI: postgresql://arena:arena_secret_123@postgres/mlflow
    command: >
      mlflow server 
      --host 0.0.0.0 
      --port 5000 
      --backend-store-uri postgresql://arena:arena_secret_123@postgres/mlflow 
      --default-artifact-root /mlflow/artifacts
    volumes:
      - mlflow_artifacts:/mlflow/artifacts
    depends_on:
      postgres:
        condition: service_healthy

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: arena-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:10.2.0
    container_name: arena-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus

volumes:
  postgres_data:
  redis_data:
  mlflow_artifacts:
  prometheus_data:
  grafana_data:
```

### 3. ì´ˆê¸°í™” SQL

```sql
-- infra/init.sql
CREATE DATABASE mlflow;

\c content_arena;

-- battles í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS battles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    config JSONB NOT NULL,
    total_rounds INTEGER NOT NULL,
    completed_rounds INTEGER DEFAULT 0,
    evasion_count INTEGER DEFAULT 0,
    detection_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- battle_rounds í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS battle_rounds (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    battle_id UUID REFERENCES battles(id),
    round_number INTEGER NOT NULL,
    original_text TEXT NOT NULL,
    evasion_text TEXT NOT NULL,
    attack_strategy VARCHAR(50) NOT NULL,
    toxic_score FLOAT NOT NULL,
    is_detected BOOLEAN NOT NULL,
    model_version VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(battle_id, round_number)
);

-- training_data í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS training_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    text TEXT NOT NULL,
    label INTEGER NOT NULL,
    source VARCHAR(50),
    battle_id UUID REFERENCES battles(id),
    created_at TIMESTAMP DEFAULT NOW()
);

-- ì¸ë±ìŠ¤
CREATE INDEX IF NOT EXISTS idx_battle_rounds_battle_id ON battle_rounds(battle_id);
CREATE INDEX IF NOT EXISTS idx_training_data_label ON training_data(label);
```

### 4. Prometheus ì„¤ì •

```yaml
# infra/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'api-service'
    static_configs:
      - targets: ['host.docker.internal:8080']

  - job_name: 'ml-inference'
    static_configs:
      - targets: ['host.docker.internal:8001']
```

### 5. ì¸í”„ë¼ ì‹¤í–‰

```bash
cd infra

# ì‹œì‘
docker compose up -d

# ìƒíƒœ í™•ì¸
docker compose ps

# ë¡œê·¸ í™•ì¸
docker compose logs -f

# ì¢…ë£Œ
docker compose down

# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ
docker compose down -v
```

---

## âœ… ì„¤ì¹˜ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. Docker & GPU
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
# âœ… GPU ì •ë³´ ì¶œë ¥

# 2. Ollama
curl http://localhost:11434/api/generate -d '{"model":"mistral:7b-instruct-v0.2-q4_K_S","prompt":"hi","stream":false}'
# âœ… ì‘ë‹µ ìˆ˜ì‹ 

# 3. Python & PyTorch
python -c "import torch; print(torch.cuda.is_available())"
# âœ… True

# 4. PostgreSQL
docker exec -it arena-postgres psql -U arena -d content_arena -c "SELECT 1"
# âœ… ì—°ê²° ì„±ê³µ

# 5. Redis
docker exec -it arena-redis redis-cli ping
# âœ… PONG

# 6. MLflow
curl http://localhost:5000/health
# âœ… ì‘ë‹µ

# 7. Go
go version
# âœ… go version go1.24.x
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ì¸ì‹ ì•ˆ ë¨

```bash
# ë“œë¼ì´ë²„ ìƒíƒœ í™•ì¸
nvidia-smi

# Docker GPU í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# nvidia-container-toolkit ì¬ì„¤ì¹˜
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Ollama ì—°ê²° ì‹¤íŒ¨

```bash
# Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
systemctl status ollama

# ìˆ˜ë™ ì‹¤í–‰
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# í¬íŠ¸ í™•ì¸
lsof -i :11434
```

### bitsandbytes ì˜¤ë¥˜ (Windows)

```bash
# CUDA ê²½ë¡œ í™•ì¸
where nvcc

# Windowsìš© ë¹Œë“œ ì‚¬ìš©
pip uninstall bitsandbytes
pip install bitsandbytes-windows
```

### PyTorch CUDA ë²„ì „ ë¶ˆì¼ì¹˜

```bash
# í˜„ì¬ ì„¤ì¹˜ëœ CUDA í™•ì¸
nvcc --version

# PyTorch CUDA ë²„ì „ í™•ì¸
python -c "import torch; print(torch.version.cuda)"

# ë§ëŠ” ë²„ì „ ì¬ì„¤ì¹˜
pip uninstall torch torchvision torchaudio
pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124
```
