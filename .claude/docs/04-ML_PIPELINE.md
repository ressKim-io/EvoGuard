# ğŸ¤– 04. ML íŒŒì´í”„ë¼ì¸ ìƒì„¸

> ê³µê²©ì ëª¨ë¸, ë°©ì–´ì ëª¨ë¸, í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ ìƒì„¸ êµ¬í˜„

---

## ğŸ“Š íŒŒì´í”„ë¼ì¸ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ML PIPELINE OVERVIEW                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë°ì´í„°ì…‹    â”‚â”€â”€â”€â–ºâ”‚   ì „ì²˜ë¦¬     â”‚â”€â”€â”€â–ºâ”‚   í•™ìŠµ       â”‚â”€â”€â”€â–ºâ”‚   í‰ê°€       â”‚
â”‚  (Jigsaw +   â”‚    â”‚  (ì •ì œ,      â”‚    â”‚  (QLoRA      â”‚    â”‚  (F1, AUC,   â”‚
â”‚   Battle)    â”‚    â”‚   í† í°í™”)    â”‚    â”‚   Fine-tune) â”‚    â”‚   ì •í™•ë„)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë°°í¬       â”‚â—„â”€â”€â”€â”‚  ë¹„êµ í‰ê°€   â”‚â—„â”€â”€â”€â”‚  ëª¨ë¸ ë“±ë¡   â”‚
â”‚  (Champion   â”‚    â”‚  (Champion   â”‚    â”‚  (MLflow     â”‚
â”‚   êµì²´)      â”‚    â”‚   vs         â”‚    â”‚   Registry)  â”‚
â”‚              â”‚    â”‚   Challenger)â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ ê³µê²©ì (Attacker) íŒŒì´í”„ë¼ì¸

### 1. ê³µê²© ì „ëµ ì•„í‚¤í…ì²˜

```python
# attacker/strategies/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List

@dataclass
class EvasionResult:
    original: str
    evasion: str
    strategy: str
    confidence: float  # ìš°íšŒ ì„±ê³µ ì˜ˆìƒ í™•ë¥ 

class AttackStrategy(ABC):
    """ê³µê²© ì „ëµ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        pass
    
    @abstractmethod
    def generate(self, text: str, num_variants: int = 1) -> List[EvasionResult]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìš°íšŒ ë³€í˜•ì„ ìƒì„±
        
        Args:
            text: ì›ë³¸ ìœ í•´ í…ìŠ¤íŠ¸
            num_variants: ìƒì„±í•  ë³€í˜• ìˆ˜
            
        Returns:
            EvasionResult ë¦¬ìŠ¤íŠ¸
        """
        pass
```

### 2. ê·œì¹™ ê¸°ë°˜ ì „ëµë“¤

```python
# attacker/strategies/rule_based.py
import random
import re
from typing import List

class UnicodeEvasionStrategy(AttackStrategy):
    """ìœ ë‹ˆì½”ë“œ ë¬¸ì ë³€í˜• ì „ëµ"""
    
    name = "unicode_evasion"
    
    # í•œê¸€ ììŒ/ëª¨ìŒ ë¶„ë¦¬ ë§¤í•‘
    JAMO_MAP = {
        'ê°€': 'ã„±ã…', 'ë‚˜': 'ã„´ã…', 'ë‹¤': 'ã„·ã…',
        'ë°”': 'ã…‚ã…', 'ì‚¬': 'ã……ã…', 'ì': 'ã…ˆã…',
        # ... í™•ì¥
    }
    
    # ìœ ì‚¬ ë¬¸ì ë§¤í•‘
    SIMILAR_CHARS = {
        'a': ['Ğ°', 'É‘', 'Î±'],  # í‚¤ë¦´, IPA, ê·¸ë¦¬ìŠ¤
        'e': ['Ğµ', 'Îµ', 'É›'],
        'o': ['Ğ¾', 'Î¿', '0'],
        'i': ['Ñ–', 'Î¹', '1', 'l'],
        # ... í™•ì¥
    }
    
    def generate(self, text: str, num_variants: int = 5) -> List[EvasionResult]:
        results = []
        
        for _ in range(num_variants):
            evasion = self._apply_random_transform(text)
            results.append(EvasionResult(
                original=text,
                evasion=evasion,
                strategy=self.name,
                confidence=0.6  # ê·œì¹™ ê¸°ë°˜ì€ ë³´í†µ ì„±ê³µë¥ 
            ))
        
        return results
    
    def _apply_random_transform(self, text: str) -> str:
        transforms = [
            self._space_insertion,
            self._jamo_decompose,
            self._similar_char_replace,
            self._zero_width_insert,
        ]
        
        # 1-3ê°œì˜ ë³€í˜•ì„ ë¬´ì‘ìœ„ ì ìš©
        num_transforms = random.randint(1, 3)
        for transform in random.sample(transforms, num_transforms):
            text = transform(text)
        
        return text
    
    def _space_insertion(self, text: str) -> str:
        """ê¸€ì ì‚¬ì´ì— ê³µë°± ì‚½ì…: ë°”ë³´ â†’ ë°” ë³´"""
        chars = list(text)
        for i in range(len(chars) - 1, 0, -1):
            if random.random() < 0.3:
                chars.insert(i, ' ')
        return ''.join(chars)
    
    def _jamo_decompose(self, text: str) -> str:
        """í•œê¸€ ìëª¨ ë¶„ë¦¬: ë°”ë³´ â†’ ã…‚ã…ã…‚ã…—"""
        result = []
        for char in text:
            if char in self.JAMO_MAP and random.random() < 0.5:
                result.append(self.JAMO_MAP[char])
            else:
                result.append(char)
        return ''.join(result)
    
    def _similar_char_replace(self, text: str) -> str:
        """ìœ ì‚¬ ë¬¸ì ì¹˜í™˜: hello â†’ hĞµllo (í‚¤ë¦´ 'Ğµ')"""
        result = []
        for char in text.lower():
            if char in self.SIMILAR_CHARS and random.random() < 0.3:
                result.append(random.choice(self.SIMILAR_CHARS[char]))
            else:
                result.append(char)
        return ''.join(result)
    
    def _zero_width_insert(self, text: str) -> str:
        """ë³´ì´ì§€ ì•ŠëŠ” ë¬¸ì ì‚½ì…"""
        zero_widths = ['\u200b', '\u200c', '\u200d', '\ufeff']
        chars = list(text)
        for i in range(len(chars) - 1, 0, -1):
            if random.random() < 0.2:
                chars.insert(i, random.choice(zero_widths))
        return ''.join(chars)


class HomoglyphStrategy(AttackStrategy):
    """ë™í˜• ë¬¸ì(Homoglyph) ì¹˜í™˜ ì „ëµ"""
    
    name = "homoglyph"
    
    # ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì ë§¤í•‘ (ë” ê´‘ë²”ìœ„)
    HOMOGLYPHS = {
        'A': ['Î‘', 'Ğ', 'áª'],
        'B': ['Î’', 'Ğ’', 'á´'],
        'C': ['Ğ¡', 'Ï¹', 'áŸ'],
        # ... ì „ì²´ ì•ŒíŒŒë²³
    }
    
    def generate(self, text: str, num_variants: int = 5) -> List[EvasionResult]:
        # êµ¬í˜„
        pass


class LeetSpeakStrategy(AttackStrategy):
    """ë¦¬íŠ¸ìŠ¤í”¼í¬ ë³€í˜•: hello â†’ h3ll0"""
    
    name = "leetspeak"
    
    LEET_MAP = {
        'a': ['4', '@'],
        'e': ['3'],
        'i': ['1', '!'],
        'o': ['0'],
        's': ['5', '$'],
        't': ['7'],
        # ...
    }
    
    def generate(self, text: str, num_variants: int = 5) -> List[EvasionResult]:
        # êµ¬í˜„
        pass
```

### 3. LLM ê¸°ë°˜ ì „ëµ

```python
# attacker/strategies/llm_based.py
import httpx
from typing import List

class LLMEvasionStrategy(AttackStrategy):
    """Ollama LLMì„ ì´ìš©í•œ ì°½ì˜ì  ìš°íšŒ"""
    
    name = "llm_evasion"
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.model = "mistral:7b-instruct-v0.2-q4_K_S"
    
    def generate(self, text: str, num_variants: int = 5) -> List[EvasionResult]:
        results = []
        
        for _ in range(num_variants):
            prompt = self._build_prompt(text)
            response = self._call_ollama(prompt)
            
            if response:
                results.append(EvasionResult(
                    original=text,
                    evasion=response,
                    strategy=self.name,
                    confidence=0.8  # LLMì€ ë” ì°½ì˜ì 
                ))
        
        return results
    
    def _build_prompt(self, text: str) -> str:
        return f"""ë‹¹ì‹ ì€ ì½˜í…ì¸  í•„í„° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì—°êµ¬ì›ì…ë‹ˆë‹¤.
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ, ìë™í™”ëœ ìš•ì„¤ í•„í„°ë¥¼ ìš°íšŒí•  ìˆ˜ ìˆëŠ” ë³€í˜•ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ í…ìŠ¤íŠ¸: "{text}"

ë³€í˜• ê·œì¹™:
1. ì˜ë¯¸ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€
2. íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨ì§€, ìœ ë‹ˆì½”ë“œ í™œìš© ê°€ëŠ¥
3. ë°œìŒì´ ë¹„ìŠ·í•œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ëŒ€ì²´ ê°€ëŠ¥
4. ë„ì–´ì“°ê¸°, ì¤„ë°”ê¿ˆ í™œìš© ê°€ëŠ¥

ë³€í˜•ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):"""
    
    def _call_ollama(self, prompt: str) -> str:
        try:
            response = httpx.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.9,  # ì°½ì˜ì„± ë†’ê²Œ
                        "top_p": 0.95,
                        "max_tokens": 100
                    }
                },
                timeout=30.0
            )
            response.raise_for_status()
            return response.json()["response"].strip()
        except Exception as e:
            print(f"Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None


class AdversarialLLMStrategy(AttackStrategy):
    """ë°©ì–´ ëª¨ë¸ì˜ ì•½ì ì„ í•™ìŠµí•˜ëŠ” ì ëŒ€ì  ì „ëµ"""
    
    name = "adversarial_llm"
    
    def __init__(self, ollama_url: str, failed_evasions: List[str] = None):
        self.ollama_url = ollama_url
        self.failed_evasions = failed_evasions or []  # íƒì§€ëœ íŒ¨í„´ë“¤
    
    def generate(self, text: str, num_variants: int = 5) -> List[EvasionResult]:
        # ì´ì „ì— íƒì§€ëœ íŒ¨í„´ì„ í”¼í•˜ë©´ì„œ ìƒˆë¡œìš´ ë³€í˜• ìƒì„±
        prompt = self._build_adversarial_prompt(text)
        # ...
    
    def _build_adversarial_prompt(self, text: str) -> str:
        failed_examples = "\n".join([f"- {e}" for e in self.failed_evasions[-10:]])
        
        return f"""ì´ì „ì— íƒì§€ëœ ìš°íšŒ íŒ¨í„´ë“¤:
{failed_examples}

ìœ„ íŒ¨í„´ë“¤ì€ ëª¨ë‘ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•˜ì„¸ìš”.

ì›ë³¸: "{text}"
ìƒˆë¡œìš´ ë³€í˜•:"""
```

### 4. ê³µê²©ì ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

```python
# attacker/orchestrator.py
from typing import List, Dict
import random

class AttackerOrchestrator:
    """ì—¬ëŸ¬ ê³µê²© ì „ëµì„ ì¡°í•©í•˜ì—¬ ì‹¤í–‰"""
    
    def __init__(self, strategies: List[AttackStrategy]):
        self.strategies = {s.name: s for s in strategies}
    
    def attack(
        self, 
        text: str, 
        strategy: str = None,
        num_variants: int = 10
    ) -> List[EvasionResult]:
        """
        ê³µê²© ì‹¤í–‰
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            strategy: íŠ¹ì • ì „ëµ ì§€ì • (Noneì´ë©´ ë¬´ì‘ìœ„)
            num_variants: ìƒì„±í•  ë³€í˜• ìˆ˜
        """
        if strategy:
            return self.strategies[strategy].generate(text, num_variants)
        
        # ì „ëµ ì¡°í•©
        results = []
        per_strategy = num_variants // len(self.strategies) + 1
        
        for s in self.strategies.values():
            results.extend(s.generate(text, per_strategy))
        
        return results[:num_variants]
    
    def evolve_strategy(self, battle_results: List[Dict]):
        """
        ë°°í‹€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì „ëµ ê°€ì¤‘ì¹˜ ì¡°ì •
        (ì–´ë–¤ ì „ëµì´ ë” íš¨ê³¼ì ì¸ì§€ í•™ìŠµ)
        """
        success_by_strategy = {}
        
        for result in battle_results:
            strategy = result["attack_strategy"]
            detected = result["is_detected"]
            
            if strategy not in success_by_strategy:
                success_by_strategy[strategy] = {"success": 0, "total": 0}
            
            success_by_strategy[strategy]["total"] += 1
            if not detected:  # íƒì§€ ì•ˆ ë¨ = ìš°íšŒ ì„±ê³µ
                success_by_strategy[strategy]["success"] += 1
        
        # ì„±ê³µë¥  ê³„ì‚° ë° ë¡œê¹…
        for strategy, stats in success_by_strategy.items():
            success_rate = stats["success"] / stats["total"] if stats["total"] > 0 else 0
            print(f"Strategy {strategy}: {success_rate:.2%} evasion rate")
```

---

## ğŸ›¡ï¸ ë°©ì–´ì (Defender) íŒŒì´í”„ë¼ì¸

### 1. ëª¨ë¸ ì•„í‚¤í…ì²˜

```python
# defender/model.py
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel

class ContentFilter:
    """ì½˜í…ì¸  í•„í„° ëª¨ë¸"""
    
    def __init__(
        self, 
        base_model: str = "bert-base-multilingual-cased",
        lora_weights: str = None,
        device: str = "cuda"
    ):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSequenceClassification.from_pretrained(
            base_model,
            num_labels=2  # 0: clean, 1: toxic
        )
        
        # LoRA ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ì ìš©
        if lora_weights:
            self.model = PeftModel.from_pretrained(self.model, lora_weights)
            self.model = self.model.merge_and_unload()  # ì¶”ë¡  ìµœì í™”
        
        self.model.to(device)
        self.model.eval()
    
    @torch.no_grad()
    def classify(self, text: str) -> Dict:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜
        
        Returns:
            {
                "toxic_score": 0.85,
                "is_toxic": True,
                "confidence": 0.85
            }
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        ).to(self.device)
        
        outputs = self.model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        
        toxic_score = probs[0][1].item()
        
        return {
            "toxic_score": toxic_score,
            "is_toxic": toxic_score > 0.5,
            "confidence": max(probs[0]).item()
        }
    
    @torch.no_grad()
    def classify_batch(self, texts: List[str], batch_size: int = 32) -> List[Dict]:
        """ë°°ì¹˜ ë¶„ë¥˜"""
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            inputs = self.tokenizer(
                batch,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.device)
            
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
            
            for j, prob in enumerate(probs):
                toxic_score = prob[1].item()
                results.append({
                    "toxic_score": toxic_score,
                    "is_toxic": toxic_score > 0.5,
                    "confidence": max(prob).item()
                })
        
        return results
```

### 2. ì¶”ë¡  API ì„œë²„

```python
# defender/api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List
import mlflow

app = FastAPI(title="Content Filter API")

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
filter_model = None

class ClassifyRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=5000)

class ClassifyBatchRequest(BaseModel):
    texts: List[str] = Field(..., min_items=1, max_items=100)

class ClassifyResponse(BaseModel):
    toxic_score: float
    is_toxic: bool
    confidence: float
    model_version: str

@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ Champion ëª¨ë¸ ë¡œë“œ"""
    global filter_model
    
    # MLflowì—ì„œ Champion ëª¨ë¸ ë¡œë“œ
    client = mlflow.tracking.MlflowClient()
    model_version = client.get_model_version_by_alias(
        name="content-filter",
        alias="champion"
    )
    
    filter_model = ContentFilter(
        base_model="bert-base-multilingual-cased",
        lora_weights=model_version.source
    )

@app.post("/classify", response_model=ClassifyResponse)
async def classify(request: ClassifyRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    result = filter_model.classify(request.text)
    result["model_version"] = get_current_version()
    return result

@app.post("/classify/batch", response_model=List[ClassifyResponse])
async def classify_batch(request: ClassifyBatchRequest):
    """ë°°ì¹˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    results = filter_model.classify_batch(request.texts)
    version = get_current_version()
    for r in results:
        r["model_version"] = version
    return results

@app.post("/reload")
async def reload_model():
    """Champion ëª¨ë¸ ì¬ë¡œë“œ (í•« ë¦¬ë¡œë“œ)"""
    global filter_model
    await load_model()
    return {"status": "reloaded"}

@app.get("/health")
async def health():
    return {"status": "healthy", "model_loaded": filter_model is not None}
```

---

## ğŸ“š í•™ìŠµ íŒŒì´í”„ë¼ì¸

### 1. ë°ì´í„°ì…‹ ì¤€ë¹„

```python
# training/data_preparation.py
from datasets import load_dataset, Dataset, concatenate_datasets
import pandas as pd
from typing import List, Dict

class DatasetPreparator:
    """í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    def __init__(self, db_connection):
        self.db = db_connection
    
    def prepare_training_data(self) -> Dataset:
        """
        í•™ìŠµ ë°ì´í„° í†µí•©
        1. Jigsaw ë² ì´ìŠ¤ ë°ì´í„°ì…‹
        2. Battleì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
        """
        # 1. Jigsaw ë°ì´í„°ì…‹ (ë² ì´ìŠ¤)
        jigsaw = self._load_jigsaw_dataset()
        
        # 2. Battle ìˆ˜ì§‘ ë°ì´í„°
        battle_data = self._load_battle_data()
        
        # 3. í†µí•©
        combined = concatenate_datasets([jigsaw, battle_data])
        
        # 4. ì…”í”Œ ë° ë¶„í• 
        combined = combined.shuffle(seed=42)
        split = combined.train_test_split(test_size=0.1)
        
        return split
    
    def _load_jigsaw_dataset(self) -> Dataset:
        """Jigsaw Toxic Comment Dataset ë¡œë“œ"""
        # Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ í•„ìš”
        # https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
        
        df = pd.read_csv("data/jigsaw_train.csv")
        
        # toxic ì»¬ëŸ¼ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•©
        df["label"] = (df[["toxic", "severe_toxic", "obscene", 
                          "threat", "insult", "identity_hate"]].sum(axis=1) > 0).astype(int)
        
        return Dataset.from_pandas(df[["comment_text", "label"]].rename(
            columns={"comment_text": "text"}
        ))
    
    def _load_battle_data(self) -> Dataset:
        """Battleì—ì„œ ìˆ˜ì§‘ëœ ìš°íšŒ íŒ¨í„´ ë°ì´í„°"""
        # ìš°íšŒ ì„±ê³µí•œ íŒ¨í„´ = toxicìœ¼ë¡œ ë ˆì´ë¸”ë§
        query = """
            SELECT evasion_text as text, 1 as label
            FROM battle_rounds
            WHERE is_detected = false
            
            UNION ALL
            
            SELECT evasion_text as text, 1 as label
            FROM battle_rounds
            WHERE is_detected = true
        """
        
        df = pd.read_sql(query, self.db)
        
        if len(df) == 0:
            return Dataset.from_dict({"text": [], "label": []})
        
        return Dataset.from_pandas(df)
    
    def _augment_data(self, dataset: Dataset) -> Dataset:
        """ë°ì´í„° ì¦ê°• (ì„ íƒì )"""
        # ë°±ë²ˆì—­, ë™ì˜ì–´ ì¹˜í™˜ ë“±
        pass


def prepare_tokenized_dataset(
    dataset: Dataset,
    tokenizer,
    max_length: int = 512
) -> Dataset:
    """í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ ìƒì„±"""
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length"
        )
    
    tokenized = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"]
    )
    
    tokenized = tokenized.rename_column("label", "labels")
    tokenized.set_format("torch")
    
    return tokenized
```

### 2. QLoRA í•™ìŠµ

```python
# training/qlora_trainer.py
import torch
import mlflow
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import numpy as np

class QLoRATrainer:
    """QLoRA Fine-tuning íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        base_model: str = "bert-base-multilingual-cased",
        output_dir: str = "./results",
        mlflow_experiment: str = "content-filter-training"
    ):
        self.base_model = base_model
        self.output_dir = output_dir
        self.mlflow_experiment = mlflow_experiment
        
        # MLflow ì„¤ì •
        mlflow.set_experiment(mlflow_experiment)
    
    def train(
        self,
        train_dataset,
        eval_dataset,
        lora_r: int = 16,
        lora_alpha: int = 32,
        epochs: int = 3,
        batch_size: int = 2,
        learning_rate: float = 2e-4
    ) -> str:
        """
        QLoRA Fine-tuning ì‹¤í–‰
        
        Returns:
            MLflow run_id
        """
        with mlflow.start_run() as run:
            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            mlflow.log_params({
                "base_model": self.base_model,
                "lora_r": lora_r,
                "lora_alpha": lora_alpha,
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": learning_rate,
                "train_samples": len(train_dataset),
                "eval_samples": len(eval_dataset)
            })
            
            # 1. 4-bit ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True
            )
            
            # 2. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(self.base_model)
            model = AutoModelForSequenceClassification.from_pretrained(
                self.base_model,
                quantization_config=bnb_config,
                device_map="auto",
                num_labels=2
            )
            
            # 3. k-bit í•™ìŠµ ì¤€ë¹„
            model = prepare_model_for_kbit_training(model)
            
            # 4. LoRA ì„¤ì •
            lora_config = LoraConfig(
                r=lora_r,
                lora_alpha=lora_alpha,
                target_modules=["query", "value", "key", "dense"],
                lora_dropout=0.05,
                bias="none",
                task_type="SEQ_CLS"
            )
            
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()
            
            # 5. í•™ìŠµ ì„¤ì •
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size * 2,
                gradient_accumulation_steps=8,
                learning_rate=learning_rate,
                warmup_ratio=0.1,
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                bf16=True,
                optim="adamw_8bit",
                gradient_checkpointing=True,
                
                # í‰ê°€ & ë¡œê¹…
                eval_strategy="steps",
                eval_steps=100,
                logging_steps=10,
                save_strategy="steps",
                save_steps=100,
                save_total_limit=3,
                load_best_model_at_end=True,
                metric_for_best_model="f1",
                
                # MLflow
                report_to="mlflow"
            )
            
            # 6. Trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=tokenizer,
                compute_metrics=self._compute_metrics
            )
            
            # 7. í•™ìŠµ ì‹¤í–‰
            trainer.train()
            
            # 8. ìµœì¢… í‰ê°€
            eval_results = trainer.evaluate()
            mlflow.log_metrics({
                f"final_{k}": v for k, v in eval_results.items()
            })
            
            # 9. ëª¨ë¸ ì €ì¥
            model_path = f"{self.output_dir}/final_model"
            trainer.save_model(model_path)
            
            # 10. MLflowì— ëª¨ë¸ ë“±ë¡
            mlflow.peft.log_model(
                model,
                artifact_path="model",
                registered_model_name="content-filter"
            )
            
            return run.info.run_id
    
    def _compute_metrics(self, eval_pred):
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        
        return {
            "accuracy": accuracy_score(labels, predictions),
            "f1": f1_score(labels, predictions, average="binary"),
            "precision": precision_score(labels, predictions, average="binary"),
            "recall": recall_score(labels, predictions, average="binary")
        }
```

### 3. ìë™ ì¬í•™ìŠµ íŠ¸ë¦¬ê±°

```python
# training/auto_retrain.py
import redis
from datetime import datetime
import threading
import time

class AutoRetrainTrigger:
    """ìë™ ì¬í•™ìŠµ íŠ¸ë¦¬ê±° ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        redis_client: redis.Redis,
        trainer: QLoRATrainer,
        data_preparator: DatasetPreparator,
        evasion_threshold: float = 0.3,  # 30% ìš°íšŒìœ¨ ë„˜ìœ¼ë©´ ì¬í•™ìŠµ
        min_new_samples: int = 100       # ìµœì†Œ ìƒˆ ìƒ˜í”Œ ìˆ˜
    ):
        self.redis = redis_client
        self.trainer = trainer
        self.data_preparator = data_preparator
        self.evasion_threshold = evasion_threshold
        self.min_new_samples = min_new_samples
        
        self._running = False
        self._lock_key = "lock:training"
    
    def start_monitoring(self):
        """ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self._running = True
        thread = threading.Thread(target=self._monitor_loop)
        thread.daemon = True
        thread.start()
    
    def _monitor_loop(self):
        """ì´ë²¤íŠ¸ í ëª¨ë‹ˆí„°ë§"""
        pubsub = self.redis.pubsub()
        pubsub.subscribe("battle_completed")
        
        for message in pubsub.listen():
            if not self._running:
                break
            
            if message["type"] == "message":
                battle_id = message["data"]
                self._check_retrain_condition(battle_id)
    
    def _check_retrain_condition(self, battle_id: str):
        """ì¬í•™ìŠµ ì¡°ê±´ í™•ì¸"""
        # 1. ìµœê·¼ ë°°í‹€ í†µê³„ ì¡°íšŒ
        stats = self._get_recent_stats()
        
        # 2. ì¡°ê±´ í™•ì¸
        if stats["evasion_rate"] > self.evasion_threshold:
            if stats["new_samples"] >= self.min_new_samples:
                self._trigger_retrain(
                    reason=f"High evasion rate: {stats['evasion_rate']:.2%}"
                )
    
    def _get_recent_stats(self) -> Dict:
        """ìµœê·¼ ë°°í‹€ í†µê³„"""
        # Redis ë˜ëŠ” DBì—ì„œ ì¡°íšŒ
        pass
    
    def _trigger_retrain(self, reason: str):
        """ì¬í•™ìŠµ ì‹¤í–‰"""
        # ë¶„ì‚° ë½ íšë“
        if not self._acquire_lock():
            print("Another training in progress, skipping")
            return
        
        try:
            print(f"Triggering retrain: {reason}")
            
            # 1. ë°ì´í„° ì¤€ë¹„
            dataset = self.data_preparator.prepare_training_data()
            
            # 2. í•™ìŠµ ì‹¤í–‰
            run_id = self.trainer.train(
                train_dataset=dataset["train"],
                eval_dataset=dataset["test"]
            )
            
            # 3. Challengerë¡œ ë“±ë¡
            self._register_as_challenger(run_id)
            
            print(f"Training completed: {run_id}")
            
        finally:
            self._release_lock()
    
    def _acquire_lock(self) -> bool:
        """ë¶„ì‚° ë½ íšë“"""
        return self.redis.set(
            self._lock_key, 
            "training", 
            nx=True, 
            ex=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        )
    
    def _release_lock(self):
        """ë¶„ì‚° ë½ í•´ì œ"""
        self.redis.delete(self._lock_key)
    
    def _register_as_challenger(self, run_id: str):
        """ìƒˆ ëª¨ë¸ì„ Challengerë¡œ ë“±ë¡"""
        client = mlflow.tracking.MlflowClient()
        
        # ìµœì‹  ë²„ì „ ê°€ì ¸ì˜¤ê¸°
        versions = client.search_model_versions(
            f"name='content-filter' and run_id='{run_id}'"
        )
        
        if versions:
            client.set_registered_model_alias(
                name="content-filter",
                alias="challenger",
                version=versions[0].version
            )
```

---

## ğŸ“ˆ í‰ê°€ ë©”íŠ¸ë¦­

### ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­

```python
# 1. ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
metrics = {
    "accuracy": 0.92,
    "f1_score": 0.87,
    "precision": 0.85,
    "recall": 0.89,
    "auc_roc": 0.94
}

# 2. ë°°í‹€ ì„±ëŠ¥ ë©”íŠ¸ë¦­
battle_metrics = {
    "detection_rate": 0.75,    # íƒì§€ìœ¨
    "evasion_rate": 0.25,      # ìš°íšŒìœ¨
    "false_positive_rate": 0.08,
    "false_negative_rate": 0.17
}

# 3. ë¼ìš´ë“œë³„ ì¶”ì´
round_metrics = [
    {"round": 1, "detection_rate": 0.60},
    {"round": 2, "detection_rate": 0.65},
    {"round": 3, "detection_rate": 0.72},
    # ...
]
```

### í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
# training/evaluation.py

class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ íŒŒì´í”„ë¼ì¸"""
    
    def evaluate_model(
        self,
        model: ContentFilter,
        test_dataset: Dataset
    ) -> Dict:
        """
        ëª¨ë¸ ì¢…í•© í‰ê°€
        """
        # 1. ê¸°ë³¸ ë¶„ë¥˜ í‰ê°€
        predictions = model.classify_batch([ex["text"] for ex in test_dataset])
        labels = [ex["label"] for ex in test_dataset]
        
        # 2. ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._compute_classification_metrics(predictions, labels)
        
        # 3. ìš°íšŒ íŒ¨í„´ í‰ê°€ (ì„ íƒì )
        evasion_metrics = self._evaluate_evasion_resistance(model)
        
        return {**metrics, **evasion_metrics}
    
    def compare_models(
        self,
        champion: ContentFilter,
        challenger: ContentFilter,
        test_dataset: Dataset
    ) -> Dict:
        """
        Champion vs Challenger ë¹„êµ
        """
        champion_metrics = self.evaluate_model(champion, test_dataset)
        challenger_metrics = self.evaluate_model(challenger, test_dataset)
        
        comparison = {
            "champion": champion_metrics,
            "challenger": challenger_metrics,
            "improvement": {
                k: challenger_metrics[k] - champion_metrics[k]
                for k in champion_metrics.keys()
            },
            "should_promote": challenger_metrics["f1"] > champion_metrics["f1"]
        }
        
        return comparison
```
