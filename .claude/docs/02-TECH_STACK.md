# ğŸ› ï¸ 02. ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸

> ê° ê¸°ìˆ ì˜ ì„ íƒ ì´ìœ , ë²„ì „, ëŒ€ì•ˆ ë¹„êµ

---

## ğŸ“Š ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½

| ë ˆì´ì–´ | ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|--------|------|------|------|
| **ì„œë¹„ìŠ¤** | Go + Gin | Go 1.24 / Gin 1.10.0 | API ì„œë²„ |
| **ML ì¶”ë¡ ** | Python + FastAPI | 3.12.x / 0.115.x | ëª¨ë¸ ì„œë¹™ |
| **ML í•™ìŠµ** | Transformers + PEFT | 4.48.x / 0.14.x | QLoRA Fine-tuning |
| **ë¡œì»¬ LLM** | Ollama + Mistral 7B | latest | ìš°íšŒ íŒ¨í„´ ìƒì„± |
| **DB** | PostgreSQL | 16.6 / 17.x | ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ |
| **ìºì‹œ** | Redis | 7.4.x | ìºì‹±, ì´ë²¤íŠ¸ í |
| **ML ì¶”ì ** | MLflow | 2.22.4 | ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ |
| **ì»¨í…Œì´ë„ˆ** | Docker + Compose | 27.4.x | ì»¨í…Œì´ë„ˆí™” |
| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | k3d (Kubernetes) | 5.7.5 | ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Docker ê¸°ë°˜) |
| **ëª¨ë‹ˆí„°ë§** | Prometheus + Grafana | 2.54.x / 11.3.x | ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì‹œê°í™” |
| **CI/CD** | GitHub Actions | - | ìë™í™” |

---

## ğŸ¹ ì„œë¹„ìŠ¤ ë ˆì´ì–´: Go + Gin

### ì™œ Goì¸ê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Go ì„ íƒ ì´ìœ                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. DevOps ì¹œí™”ë„ (K8s, Prometheus, Docker ëª¨ë‘ Go ê¸°ë°˜)        â”‚
â”‚  2. ì»¨í…Œì´ë„ˆ í¬ê¸° (10-20MB vs Java 200MB+)                     â”‚
â”‚  3. ë¹ ë¥¸ ì‹œì‘ ì‹œê°„ (ì½œë“œ ìŠ¤íƒ€íŠ¸ ìµœì†Œí™”)                         â”‚
â”‚  4. ë‹¨ì¼ ë°”ì´ë„ˆë¦¬ ë°°í¬                                          â”‚
â”‚  5. ë¶€íŠ¸ìº í”„ í”„ë¡œì íŠ¸ì™€ ì°¨ë³„í™” (JavaëŠ” ë‹¤ë¥¸ íŒ€ì› ë‹´ë‹¹)          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Go vs Java vs Node.js ë¹„êµ

| í•­ëª© | Go | Java | Node.js |
|------|-----|------|---------|
| ì»¨í…Œì´ë„ˆ í¬ê¸° | 10-20MB | 200MB+ | 100MB+ |
| ì½œë“œ ìŠ¤íƒ€íŠ¸ | ~10ms | ~1000ms | ~100ms |
| ë™ì‹œì„± ëª¨ë¸ | ê³ ë£¨í‹´ (ê²½ëŸ‰) | ìŠ¤ë ˆë“œ | ì´ë²¤íŠ¸ ë£¨í”„ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë‚®ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| DevOps ìƒíƒœê³„ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| ML ìƒíƒœê³„ ì—°ë™ | â­â­ | â­â­â­ | â­â­ |

### Gin Framework ì„ íƒ ì´ìœ 

```go
// Ginì˜ ì¥ì  ì½”ë“œ ì˜ˆì‹œ

// 1. ë¹ ë¥¸ ë¼ìš°íŒ… (httprouter ê¸°ë°˜)
router := gin.Default()
router.GET("/battles/:id", getBattle)
router.POST("/battles", createBattle)

// 2. ë‚´ì¥ ë¯¸ë“¤ì›¨ì–´
router.Use(gin.Logger())
router.Use(gin.Recovery())
router.Use(cors.Default())

// 3. JSON ë°”ì¸ë”©/ê²€ì¦
type CreateBattleRequest struct {
    Rounds   int    `json:"rounds" binding:"required,min=1,max=1000"`
    Strategy string `json:"strategy" binding:"required,oneof=unicode llm homoglyph"`
}

func createBattle(c *gin.Context) {
    var req CreateBattleRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{"error": err.Error()})
        return
    }
    // ...
}

// 4. ê·¸ë£¹ ë¼ìš°íŒ…
v1 := router.Group("/api/v1")
{
    battles := v1.Group("/battles")
    {
        battles.GET("", listBattles)
        battles.POST("", createBattle)
        battles.GET("/:id", getBattle)
    }
}
```

### ì£¼ìš” Go íŒ¨í‚¤ì§€

```go
// go.mod
module content-arena/api-service

go 1.24

require (
    github.com/gin-gonic/gin v1.10.0          // ì›¹ í”„ë ˆì„ì›Œí¬
    github.com/gin-contrib/cors v1.7.3        // CORS
    gorm.io/gorm v1.25.12                     // ORM
    gorm.io/driver/postgres v1.5.9            // PostgreSQL ë“œë¼ì´ë²„
    github.com/redis/go-redis/v9 v9.7.0       // Redis í´ë¼ì´ì–¸íŠ¸
    github.com/prometheus/client_golang v1.20.0 // Prometheus ë©”íŠ¸ë¦­
    github.com/google/uuid v1.6.0             // UUID ìƒì„±
    github.com/spf13/viper v1.19.0            // ì„¤ì • ê´€ë¦¬
    go.uber.org/zap v1.27.0                   // êµ¬ì¡°í™”ëœ ë¡œê¹…
)
```

---

## ğŸ ML ë ˆì´ì–´: Python

### Python í™˜ê²½

```
Python 3.12.x (CUDA í˜¸í™˜ì„± í™•ì¸ë¨)

ì£¼ìš” íŒ¨í‚¤ì§€ (ë³´ìˆ˜ì  - ì•ˆì •ì„± ìš°ì„ ):
â”œâ”€â”€ torch==2.5.1+cu124          # PyTorch (CUDA 12.4)
â”œâ”€â”€ transformers==4.48.3        # Hugging Face Transformers
â”œâ”€â”€ peft==0.14.0                # LoRA/QLoRA
â”œâ”€â”€ bitsandbytes==0.49.1        # 4-bit ì–‘ìí™”
â”œâ”€â”€ accelerate==1.5.2           # ë¶„ì‚°/í˜¼í•© ì •ë°€ë„ í•™ìŠµ
â”œâ”€â”€ datasets==3.2.0             # ë°ì´í„°ì…‹ ì²˜ë¦¬
â”œâ”€â”€ safetensors==0.4.5          # ëª¨ë¸ ì €ì¥ í¬ë§·
â”œâ”€â”€ mlflow==2.22.4              # ì‹¤í—˜ ì¶”ì 
â”œâ”€â”€ fastapi==0.115.6            # ì¶”ë¡  API
â”œâ”€â”€ pydantic==2.10.3            # ë°ì´í„° ê²€ì¦
â”œâ”€â”€ uvicorn==0.32.1             # ASGI ì„œë²„
â”œâ”€â”€ httpx==0.28.1               # HTTP í´ë¼ì´ì–¸íŠ¸
â””â”€â”€ redis==5.2.1                # Redis í´ë¼ì´ì–¸íŠ¸

âš ï¸ Breaking Changes ì£¼ì˜:
â”œâ”€â”€ Go 1.23.xëŠ” EOL (2025ë…„ 8ì›”) - 1.24+ ì‚¬ìš©
â”œâ”€â”€ Python 3.9ëŠ” MLflow 3.x, PEFT 0.18+ ë¯¸ì§€ì›
â”œâ”€â”€ PEFT < 0.18ì€ Transformers v5 í˜¸í™˜ ë¶ˆê°€
â””â”€â”€ MLflow 2â†’3 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ DB ìŠ¤í‚¤ë§ˆ ë³€ê²½ í•„ìš”
```

### FastAPI ì„ íƒ ì´ìœ 

```python
# FastAPIì˜ ì¥ì 

# 1. ìë™ API ë¬¸ì„œí™” (Swagger UI)
from fastapi import FastAPI
app = FastAPI(
    title="Content Filter Inference API",
    description="ì½˜í…ì¸  ë¶„ë¥˜ ì¶”ë¡  ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# 2. Pydantic ê²€ì¦
from pydantic import BaseModel, Field

class ClassifyRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=5000)
    model_alias: str = Field(default="champion")

class ClassifyResponse(BaseModel):
    toxic_score: float
    is_toxic: bool
    confidence: float
    model_version: str

# 3. ë¹„ë™ê¸° ì§€ì›
@app.post("/classify", response_model=ClassifyResponse)
async def classify(request: ClassifyRequest):
    result = await inference_service.classify(request.text)
    return result

# 4. ì˜ì¡´ì„± ì£¼ì…
from fastapi import Depends

def get_model_service():
    return ModelService.get_instance()

@app.post("/classify")
async def classify(
    request: ClassifyRequest,
    model: ModelService = Depends(get_model_service)
):
    return await model.classify(request.text)
```

---

## ğŸ¦™ ë¡œì»¬ LLM: Ollama + Mistral

### ì™œ Ollamaì¸ê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama ì„ íƒ ì´ìœ                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. ì›í´ë¦­ ì„¤ì¹˜ (curl -fsSL https://ollama.com/install.sh | sh)â”‚
â”‚  2. ëª¨ë¸ ê´€ë¦¬ ê°„í¸ (ollama pull, ollama run)                   â”‚
â”‚  3. REST API ê¸°ë³¸ ì œê³µ (localhost:11434)                       â”‚
â”‚  4. ì–‘ìí™” ëª¨ë¸ ìµœì í™”                                          â”‚
â”‚  5. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìë™í™”                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ollama vs ëŒ€ì•ˆ ë¹„êµ

| í•­ëª© | Ollama | vLLM | llama.cpp | HuggingFace TGI |
|------|--------|------|-----------|-----------------|
| ì„¤ì¹˜ ë‚œì´ë„ | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­ |
| API ê¸°ë³¸ ì œê³µ | âœ… | âœ… | âŒ | âœ… |
| GPU ìµœì í™” | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| ë©”ëª¨ë¦¬ íš¨ìœ¨ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| ë¬¸ì„œ/ì»¤ë®¤ë‹ˆí‹° | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

### Mistral 7B ëª¨ë¸ ì„ íƒ

```bash
# ì‚¬ìš©í•  ëª¨ë¸
ollama pull mistral:7b-instruct-v0.2-q4_K_S

# ëª¨ë¸ ìŠ¤í™
# - íŒŒë¼ë¯¸í„°: 7B
# - ì–‘ìí™”: 4-bit (Q4_K_S)
# - VRAM ìš”êµ¬ëŸ‰: ~4-5GB
# - ì¶”ë¡  ì†ë„: ~30 tokens/sec (4060Ti ê¸°ì¤€)
```

### 4060Ti VRAM 8GBë¡œ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì–‘ìí™” | VRAM ì‚¬ìš©ëŸ‰ | ì¶”ì²œë„ |
|------|----------|--------|-------------|--------|
| Mistral 7B Q4 | 7B | 4-bit | ~4-5GB | â­â­â­â­â­ |
| Llama 3.2 3B | 3B | FP16 | ~6GB | â­â­â­â­ |
| Llama 3.1 8B Q4 | 8B | 4-bit | ~5-6GB | â­â­â­â­ |
| Qwen 2.5 7B Q4 | 7B | 4-bit | ~5GB | â­â­â­â­ |
| Phi-3 Mini | 3.8B | 4-bit | ~3GB | â­â­â­ |

---

## ğŸ¯ QLoRA Fine-tuning

### QLoRA í•µì‹¬ ê°œë…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QLoRA ë©”ëª¨ë¦¬ ì ˆê°                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Full Fine-tuning (7B ëª¨ë¸):                                   â”‚
â”‚  â”œâ”€â”€ ëª¨ë¸ ê°€ì¤‘ì¹˜: 7B Ã— 2 bytes = 14GB                          â”‚
â”‚  â”œâ”€â”€ ê·¸ë˜ë””ì–¸íŠ¸:  7B Ã— 4 bytes = 28GB                          â”‚
â”‚  â”œâ”€â”€ ì˜µí‹°ë§ˆì´ì €: 7B Ã— 8 bytes = 56GB                           â”‚
â”‚  â””â”€â”€ ì´ í•„ìš”:    ~100GB VRAM                                   â”‚
â”‚                                                                 â”‚
â”‚  QLoRA (4-bit + LoRA):                                         â”‚
â”‚  â”œâ”€â”€ ëª¨ë¸ ê°€ì¤‘ì¹˜: 7B Ã— 0.5 bytes = 3.5GB (4-bit ì–‘ìí™”)        â”‚
â”‚  â”œâ”€â”€ LoRA íŒŒë¼ë¯¸í„°: ~10M Ã— 2 bytes = 20MB (0.1% í•™ìŠµ)          â”‚
â”‚  â”œâ”€â”€ ê·¸ë˜ë””ì–¸íŠ¸: 10M Ã— 4 bytes = 40MB                          â”‚
â”‚  â””â”€â”€ ì´ í•„ìš”:    ~6-8GB VRAM âœ…                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### QLoRA ì„¤ì • ì˜ˆì‹œ

```python
from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

# 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",              # NormalFloat4 (QLoRA í•µì‹¬)
    bnb_4bit_compute_dtype=torch.bfloat16,  # ì—°ì‚° ì‹œ bf16 ì‚¬ìš©
    bnb_4bit_use_double_quant=True          # ì´ì¤‘ ì–‘ìí™” (ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆê°)
)

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    quantization_config=bnb_config,
    device_map="auto",
    num_labels=2
)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,                                    # LoRA rank (ì‘ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
    lora_alpha=32,                           # ìŠ¤ì¼€ì¼ë§ íŒ©í„° (ë³´í†µ r * 2)
    target_modules=["query", "value"],       # ì ìš©í•  ë ˆì´ì–´
    lora_dropout=0.05,                       # ë“œë¡­ì•„ì›ƒ
    bias="none",
    task_type="SEQ_CLS"
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
# ì¶œë ¥: trainable params: 294,912 || all params: 109,482,240 || trainable%: 0.27%
```

### 8GB VRAMì—ì„œ ì•ˆì „í•œ í•™ìŠµ ì„¤ì •

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    
    # ë°°ì¹˜ ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
    per_device_train_batch_size=2,          # ì‘ì€ ë°°ì¹˜
    gradient_accumulation_steps=8,          # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ ì‹¤íš¨ ë°°ì¹˜ = 16
    
    # ì •ë°€ë„ ì„¤ì •
    fp16=False,                             # 4060TiëŠ” bf16 ë” íš¨ìœ¨ì 
    bf16=True,
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    optim="adamw_8bit",                     # 8-bit ì˜µí‹°ë§ˆì´ì €
    gradient_checkpointing=True,            # ë©”ëª¨ë¦¬-ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
    
    # í•™ìŠµ ì„¤ì •
    learning_rate=2e-4,
    num_train_epochs=3,
    warmup_ratio=0.1,
    
    # ë¡œê¹…
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
)
```

---

## ğŸ“Š MLflow

### MLflow êµ¬ì„±ìš”ì†Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MLflow êµ¬ì„±                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Tracking                                                    â”‚
â”‚     - ì‹¤í—˜ íŒŒë¼ë¯¸í„° ê¸°ë¡                                        â”‚
â”‚     - ë©”íŠ¸ë¦­ ê¸°ë¡ (loss, accuracy, f1)                         â”‚
â”‚     - ì•„í‹°íŒ©íŠ¸ ì €ì¥ (ëª¨ë¸, ê·¸ë˜í”„)                              â”‚
â”‚                                                                 â”‚
â”‚  2. Model Registry                                              â”‚
â”‚     - ëª¨ë¸ ë²„ì „ ê´€ë¦¬                                            â”‚
â”‚     - Alias ê¸°ë°˜ ë°°í¬ (champion, challenger)                    â”‚
â”‚     - Stage ê´€ë¦¬ (Staging â†’ Production)                         â”‚
â”‚                                                                 â”‚
â”‚  3. Projects                                                    â”‚
â”‚     - ì¬í˜„ ê°€ëŠ¥í•œ í•™ìŠµ í™˜ê²½                                     â”‚
â”‚     - MLproject íŒŒì¼ë¡œ ì‹¤í–‰ ì •ì˜                                â”‚
â”‚                                                                 â”‚
â”‚  4. Models                                                      â”‚
â”‚     - ëª¨ë¸ ì„œë¹™ (mlflow models serve)                           â”‚
â”‚     - ë‹¤ì–‘í•œ í”Œë ˆì´ë²„ ì§€ì› (sklearn, pytorch, ...)              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MLflow ì‚¬ìš© ì˜ˆì‹œ

```python
import mlflow
from mlflow.tracking import MlflowClient

# 1. ì‹¤í—˜ ì¶”ì 
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("content-filter-training")

with mlflow.start_run(run_name="bert-qlora-v1"):
    # íŒŒë¼ë¯¸í„° ë¡œê¹…
    mlflow.log_params({
        "model": "bert-base-uncased",
        "lora_r": 16,
        "lora_alpha": 32,
        "batch_size": 2,
        "learning_rate": 2e-4
    })
    
    # ë©”íŠ¸ë¦­ ë¡œê¹…
    mlflow.log_metrics({
        "train_loss": 0.35,
        "eval_loss": 0.42,
        "eval_f1": 0.87,
        "eval_accuracy": 0.89
    })
    
    # ëª¨ë¸ ì €ì¥
    mlflow.pytorch.log_model(model, "model")

# 2. Model Registry
client = MlflowClient()

# ëª¨ë¸ ë“±ë¡
model_uri = f"runs:/{run.info.run_id}/model"
mv = client.create_model_version(
    name="content-filter",
    source=model_uri,
    run_id=run.info.run_id
)

# Alias ì„¤ì • (champion/challenger)
client.set_registered_model_alias(
    name="content-filter",
    alias="challenger",
    version=mv.version
)

# 3. Champion/Challenger ë¹„êµ í›„ ìŠ¹ê²©
challenger_metrics = client.get_run(challenger_run_id).data.metrics
champion_metrics = client.get_run(champion_run_id).data.metrics

if challenger_metrics["eval_f1"] > champion_metrics["eval_f1"]:
    # Challengerë¥¼ Championìœ¼ë¡œ ìŠ¹ê²©
    client.set_registered_model_alias(
        name="content-filter",
        alias="champion",
        version=mv.version
    )
```

---

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL

### ì„ íƒ ì´ìœ 

```
PostgreSQL ì„ íƒ ì´ìœ :
â”œâ”€â”€ JSONB ì§€ì› (ì„¤ì •, ë©”íƒ€ë°ì´í„° ì €ì¥ì— ìœ ìš©)
â”œâ”€â”€ ì „ë¬¸ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê¸°ëŠ¥)
â”œâ”€â”€ í™•ì¥ì„± (íŒŒí‹°ì…”ë‹, ë ˆí”Œë¦¬ì¼€ì´ì…˜)
â”œâ”€â”€ Go/Python ë“œë¼ì´ë²„ ì„±ìˆ™
â””â”€â”€ K8s ìš´ì˜ ê²½í—˜ ì¶•ì  (wealist í”„ë¡œì íŠ¸)
```

---

## ğŸš€ Redis

### ì‚¬ìš© ìš©ë„

```
Redis ì‚¬ìš© ìš©ë„:
â”œâ”€â”€ ìºì‹±
â”‚   â”œâ”€â”€ ë¶„ë¥˜ ê²°ê³¼ ìºì‹± (ê°™ì€ í…ìŠ¤íŠ¸ ì¬ìš”ì²­ ì‹œ)
â”‚   â”œâ”€â”€ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìºì‹±
â”‚   â””â”€â”€ ë°°í‹€ ì§„í–‰ ìƒíƒœ
â”‚
â”œâ”€â”€ ì´ë²¤íŠ¸/ë©”ì‹œì§€ í
â”‚   â”œâ”€â”€ ì¬í•™ìŠµ íŠ¸ë¦¬ê±° ì´ë²¤íŠ¸
â”‚   â”œâ”€â”€ ëª¨ë¸ êµì²´ ì•Œë¦¼
â”‚   â””â”€â”€ ë°°í‹€ ì™„ë£Œ ì´ë²¤íŠ¸
â”‚
â””â”€â”€ ë¶„ì‚° ë½
    â””â”€â”€ ì¬í•™ìŠµ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
```

---

## ğŸ“¦ ì»¨í…Œì´ë„ˆ & ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

### Docker Compose (ê°œë°œ í™˜ê²½)

```yaml
# docker-compose.yml
version: "3.9"

services:
  api:
    build: ./api-service
    ports:
      - "8080:8080"
    environment:
      - DB_HOST=postgres
      - REDIS_HOST=redis
    depends_on:
      - postgres
      - redis

  ml-inference:
    build: ./ml-service
    ports:
      - "8001:8001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  postgres:
    image: postgres:16
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=content_arena
      - POSTGRES_USER=arena
      - POSTGRES_PASSWORD=secret

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    ports:
      - "5000:5000"
    command: mlflow server --host 0.0.0.0 --backend-store-uri postgresql://arena:secret@postgres/mlflow

  prometheus:
    image: prom/prometheus:v2.48.0
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:10.2.0
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  postgres_data:
  redis_data:
  grafana_data:
```

### k3d (ë¡œì»¬ ê°œë°œ/ë°ëª¨)

```
k3d ì„ íƒ ì´ìœ :
â”œâ”€â”€ Docker ì»¨í…Œì´ë„ˆë¡œ k3s ì‹¤í–‰ (ë³„ë„ VM ë¶ˆí•„ìš”)
â”œâ”€â”€ í´ëŸ¬ìŠ¤í„° ìƒì„±/ì‚­ì œ ì´ˆë‹¨ìœ„ (ê°œë°œ ì¤‘ ë¹ ë¥¸ ë°˜ë³µ)
â”œâ”€â”€ Dockerë§Œ ìˆìœ¼ë©´ OK (WSL2 + Docker Desktop í™˜ê²½ ìµœì )
â”œâ”€â”€ í’€ K8s API í˜¸í™˜ (k3s ê¸°ë°˜)
â”œâ”€â”€ í¬íŠ¸ ë§¤í•‘, ë³¼ë¥¨ ë§ˆìš´íŠ¸ Docker ì¹œí™”ì 
â””â”€â”€ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ë™ì‹œ ìš´ì˜ ê°€ëŠ¥

# ì„¤ì¹˜
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

# í´ëŸ¬ìŠ¤í„° ìƒì„± (í¬íŠ¸ ë§¤í•‘ í¬í•¨)
k3d cluster create content-arena \
  --port "8080:80@loadbalancer" \
  --port "8443:443@loadbalancer" \
  --agents 1

# í´ëŸ¬ìŠ¤í„° ì‚­ì œ
k3d cluster delete content-arena

# kubectl ì»¨í…ìŠ¤íŠ¸ ìë™ ì„¤ì •ë¨
kubectl get nodes
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§: Prometheus + Grafana

### ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­

```
# ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
content_arena_battles_total{status="completed|failed"}
content_arena_rounds_total{detected="true|false"}
content_arena_detection_rate
content_arena_evasion_rate

# ëª¨ë¸ ë©”íŠ¸ë¦­
content_arena_model_inference_duration_seconds
content_arena_model_version{alias="champion|challenger"}
content_arena_model_f1_score

# ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
content_arena_api_request_duration_seconds
content_arena_api_requests_total{path, method, status}
content_arena_gpu_memory_used_bytes
```

---

## ğŸ”„ CI/CD: GitHub Actions

### ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test-api:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v5
        with:
          go-version: '1.24'
      - run: cd api-service && go test ./...

  test-ml:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - run: |
          cd ml-service
          pip install -r requirements-test.txt
          pytest

  build:
    needs: [test-api, test-ml]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: docker/build-push-action@v5
        with:
          context: ./api-service
          push: true
          tags: ghcr.io/${{ github.repository }}/api:${{ github.sha }}
```
