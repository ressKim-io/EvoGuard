# âš™ï¸ 05. MLOps íŒŒì´í”„ë¼ì¸

> Champion/Challenger íŒ¨í„´, ìë™ ë°°í¬, ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

---

## ğŸ¯ MLOps ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MLOps LIFECYCLE                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚  â”‚  Data    â”‚â”€â”€â”€â–ºâ”‚  Train   â”‚â”€â”€â”€â–ºâ”‚ Register â”‚â”€â”€â”€â–ºâ”‚ Compare  â”‚         â”‚
    â”‚  â”‚ Pipeline â”‚    â”‚ Pipeline â”‚    â”‚ (MLflow) â”‚    â”‚ (A/B)    â”‚         â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                                                       â”‚               â”‚
    â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                       â–¼                               â–¼
    â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                              â”‚   Promote    â”‚                â”‚   Reject     â”‚
    â”‚                              â”‚ (Champion)   â”‚                â”‚ (Archive)    â”‚
    â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                     â”‚                                       â”‚
    â”‚                                     â–¼                                       â”‚
    â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
    â”‚                              â”‚   Deploy     â”‚                              â”‚
    â”‚                              â”‚ (Hot Reload) â”‚                              â”‚
    â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
    â”‚                                     â”‚                                       â”‚
    â”‚                                     â–¼                                       â”‚
    â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
    â”‚                              â”‚   Monitor    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                              â”‚ (Metrics)    â”‚        Feedback Loop        â”‚
    â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
    â”‚                                                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ† Champion/Challenger íŒ¨í„´

### ê°œë…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CHAMPION/CHALLENGER íŒ¨í„´                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Champion (Production)                                          â”‚
â”‚  â”œâ”€â”€ í˜„ì¬ í”„ë¡œë•ì…˜ íŠ¸ë˜í”½ 100% ì²˜ë¦¬                            â”‚
â”‚  â”œâ”€â”€ ê²€ì¦ëœ ì„±ëŠ¥ (F1: 0.85)                                    â”‚
â”‚  â””â”€â”€ MLflow alias: "champion"                                   â”‚
â”‚                                                                 â”‚
â”‚  Challenger (Shadow)                                            â”‚
â”‚  â”œâ”€â”€ í”„ë¡œë•ì…˜ íŠ¸ë˜í”½ ì²˜ë¦¬ ì•ˆ í•¨                                â”‚
â”‚  â”œâ”€â”€ Shadow ëª¨ë“œë¡œ ë¡œê¹…ë§Œ                                      â”‚
â”‚  â”œâ”€â”€ Championê³¼ ë™ì¼ ì…ë ¥ìœ¼ë¡œ í‰ê°€                              â”‚
â”‚  â””â”€â”€ MLflow alias: "challenger"                                 â”‚
â”‚                                                                 â”‚
â”‚  ìŠ¹ê²© ì¡°ê±´:                                                     â”‚
â”‚  â”œâ”€â”€ F1 Score > Champion + 0.01 (1% ê°œì„ )                      â”‚
â”‚  â”œâ”€â”€ Precision >= Champion (ì •ë°€ë„ ìœ ì§€)                        â”‚
â”‚  â””â”€â”€ ìµœì†Œ 1000ê±´ í‰ê°€ ì™„ë£Œ                                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MLflow Model Registry ì„¤ì •

```python
# mlops/model_registry.py
import mlflow
from mlflow.tracking import MlflowClient
from typing import Optional, Dict, List
from dataclasses import dataclass

@dataclass
class ModelVersion:
    name: str
    version: int
    alias: str
    run_id: str
    metrics: Dict[str, float]
    source: str

class ModelRegistry:
    """MLflow Model Registry ê´€ë¦¬"""
    
    def __init__(self, tracking_uri: str = "http://localhost:5000"):
        mlflow.set_tracking_uri(tracking_uri)
        self.client = MlflowClient()
        self.model_name = "content-filter"
    
    def get_champion(self) -> Optional[ModelVersion]:
        """í˜„ì¬ Champion ëª¨ë¸ ì¡°íšŒ"""
        try:
            version = self.client.get_model_version_by_alias(
                name=self.model_name,
                alias="champion"
            )
            return self._to_model_version(version, "champion")
        except mlflow.exceptions.MlflowException:
            return None
    
    def get_challenger(self) -> Optional[ModelVersion]:
        """í˜„ì¬ Challenger ëª¨ë¸ ì¡°íšŒ"""
        try:
            version = self.client.get_model_version_by_alias(
                name=self.model_name,
                alias="challenger"
            )
            return self._to_model_version(version, "challenger")
        except mlflow.exceptions.MlflowException:
            return None
    
    def register_challenger(self, run_id: str) -> ModelVersion:
        """ìƒˆ ëª¨ë¸ì„ Challengerë¡œ ë“±ë¡"""
        # 1. ëª¨ë¸ ë²„ì „ ìƒì„±
        model_uri = f"runs:/{run_id}/model"
        mv = mlflow.register_model(model_uri, self.model_name)
        
        # 2. Challenger alias ì„¤ì •
        self.client.set_registered_model_alias(
            name=self.model_name,
            alias="challenger",
            version=mv.version
        )
        
        return self._to_model_version(
            self.client.get_model_version(self.model_name, mv.version),
            "challenger"
        )
    
    def promote_challenger(self) -> bool:
        """
        Challengerë¥¼ Championìœ¼ë¡œ ìŠ¹ê²©
        
        Returns:
            True if promoted, False otherwise
        """
        challenger = self.get_challenger()
        if not challenger:
            return False
        
        # 1. ê¸°ì¡´ Champion alias ì œê±° (ìˆìœ¼ë©´)
        champion = self.get_champion()
        if champion:
            self.client.delete_registered_model_alias(
                name=self.model_name,
                alias="champion"
            )
        
        # 2. Challengerë¥¼ Championìœ¼ë¡œ
        self.client.set_registered_model_alias(
            name=self.model_name,
            alias="champion",
            version=challenger.version
        )
        
        # 3. Challenger alias ì œê±°
        self.client.delete_registered_model_alias(
            name=self.model_name,
            alias="challenger"
        )
        
        return True
    
    def reject_challenger(self):
        """Challenger ê±°ë¶€ (ë³´ê´€)"""
        challenger = self.get_challenger()
        if challenger:
            # alias ì œê±°ë§Œ (ë²„ì „ì€ ìœ ì§€)
            self.client.delete_registered_model_alias(
                name=self.model_name,
                alias="challenger"
            )
            # íƒœê·¸ë¡œ ê±°ë¶€ ì‚¬ìœ  ê¸°ë¡
            self.client.set_model_version_tag(
                name=self.model_name,
                version=challenger.version,
                key="status",
                value="rejected"
            )
    
    def _to_model_version(self, mv, alias: str) -> ModelVersion:
        """MLflow ModelVersionì„ ë‚´ë¶€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        run = self.client.get_run(mv.run_id)
        return ModelVersion(
            name=mv.name,
            version=int(mv.version),
            alias=alias,
            run_id=mv.run_id,
            metrics=run.data.metrics,
            source=mv.source
        )
```

### ë¹„êµ í‰ê°€ ì‹œìŠ¤í…œ

```python
# mlops/evaluator.py
from typing import Dict, Tuple
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score

class ChampionChallengerEvaluator:
    """Champion vs Challenger ë¹„êµ í‰ê°€"""
    
    def __init__(
        self,
        model_registry: ModelRegistry,
        defender_service,  # API í´ë¼ì´ì–¸íŠ¸
        min_samples: int = 1000,
        improvement_threshold: float = 0.01
    ):
        self.registry = model_registry
        self.defender = defender_service
        self.min_samples = min_samples
        self.improvement_threshold = improvement_threshold
    
    def evaluate_on_test_set(
        self,
        test_data: List[Dict]  # [{"text": str, "label": int}, ...]
    ) -> Dict:
        """
        í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ Champion/Challenger ë¹„êµ
        """
        texts = [d["text"] for d in test_data]
        labels = [d["label"] for d in test_data]
        
        # 1. Champion í‰ê°€
        champion_preds = self._evaluate_model("champion", texts)
        champion_metrics = self._compute_metrics(champion_preds, labels)
        
        # 2. Challenger í‰ê°€
        challenger_preds = self._evaluate_model("challenger", texts)
        challenger_metrics = self._compute_metrics(challenger_preds, labels)
        
        # 3. ë¹„êµ
        comparison = {
            "champion": champion_metrics,
            "challenger": challenger_metrics,
            "improvement": {
                k: challenger_metrics[k] - champion_metrics[k]
                for k in champion_metrics
            },
            "samples_evaluated": len(test_data)
        }
        
        # 4. ìŠ¹ê²© íŒë‹¨
        comparison["should_promote"] = self._should_promote(
            champion_metrics, challenger_metrics
        )
        
        return comparison
    
    def _evaluate_model(self, alias: str, texts: List[str]) -> List[int]:
        """ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        results = self.defender.classify_batch(texts, model_alias=alias)
        return [1 if r["is_toxic"] else 0 for r in results]
    
    def _compute_metrics(
        self, 
        predictions: List[int], 
        labels: List[int]
    ) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            "f1": f1_score(labels, predictions, average="binary"),
            "precision": precision_score(labels, predictions, average="binary"),
            "recall": recall_score(labels, predictions, average="binary"),
            "accuracy": np.mean(np.array(predictions) == np.array(labels))
        }
    
    def _should_promote(
        self,
        champion: Dict[str, float],
        challenger: Dict[str, float]
    ) -> bool:
        """ìŠ¹ê²© ì—¬ë¶€ íŒë‹¨"""
        # ì¡°ê±´ 1: F1 ê°œì„ 
        f1_improved = (
            challenger["f1"] - champion["f1"] > self.improvement_threshold
        )
        
        # ì¡°ê±´ 2: Precision ìœ ì§€ ë˜ëŠ” ê°œì„ 
        precision_ok = challenger["precision"] >= champion["precision"] - 0.02
        
        return f1_improved and precision_ok


class ShadowEvaluator:
    """ì‹¤ì‹œê°„ Shadow í‰ê°€ (í”„ë¡œë•ì…˜ íŠ¸ë˜í”½ í™œìš©)"""
    
    def __init__(self, redis_client, model_registry: ModelRegistry):
        self.redis = redis_client
        self.registry = model_registry
        self.shadow_results_key = "shadow:results"
    
    def record_shadow_result(
        self,
        text: str,
        champion_result: Dict,
        challenger_result: Dict,
        ground_truth: int = None
    ):
        """Shadow í‰ê°€ ê²°ê³¼ ê¸°ë¡"""
        result = {
            "text_hash": hash(text),
            "champion_toxic": champion_result["is_toxic"],
            "champion_score": champion_result["toxic_score"],
            "challenger_toxic": challenger_result["is_toxic"],
            "challenger_score": challenger_result["toxic_score"],
            "ground_truth": ground_truth,
            "timestamp": datetime.now().isoformat()
        }
        
        self.redis.lpush(self.shadow_results_key, json.dumps(result))
        self.redis.ltrim(self.shadow_results_key, 0, 10000)  # ìµœê·¼ 10000ê±´ ìœ ì§€
    
    def get_shadow_comparison(self) -> Dict:
        """Shadow í‰ê°€ í†µê³„"""
        results = [
            json.loads(r) 
            for r in self.redis.lrange(self.shadow_results_key, 0, -1)
        ]
        
        if not results:
            return {"error": "No shadow results"}
        
        # ì¼ì¹˜ìœ¨ ê³„ì‚°
        agreement = sum(
            1 for r in results 
            if r["champion_toxic"] == r["challenger_toxic"]
        ) / len(results)
        
        # Ground truthê°€ ìˆëŠ” ê²½ìš° ì •í™•ë„ ë¹„êµ
        labeled = [r for r in results if r["ground_truth"] is not None]
        if labeled:
            champion_acc = sum(
                1 for r in labeled 
                if r["champion_toxic"] == r["ground_truth"]
            ) / len(labeled)
            challenger_acc = sum(
                1 for r in labeled 
                if r["challenger_toxic"] == r["ground_truth"]
            ) / len(labeled)
        else:
            champion_acc = challenger_acc = None
        
        return {
            "total_samples": len(results),
            "agreement_rate": agreement,
            "champion_accuracy": champion_acc,
            "challenger_accuracy": challenger_acc,
            "labeled_samples": len(labeled)
        }
```

---

## ğŸš€ ìë™ ë°°í¬ íŒŒì´í”„ë¼ì¸

### ë°°í¬ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AUTO DEPLOYMENT FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Training          MLflow           Evaluator         Deployer
     â”‚                â”‚                  â”‚                â”‚
     â”‚  train_completeâ”‚                  â”‚                â”‚
     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚                â”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚  new_challenger  â”‚                â”‚
     â”‚                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚                  â”‚ evaluate()     â”‚
     â”‚                â”‚                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚                  â”‚ compare()      â”‚
     â”‚                â”‚                  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚   if better:     â”‚                â”‚
     â”‚                â”‚   promote()      â”‚                â”‚
     â”‚                â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚
     â”‚                â”‚                  â”‚                â”‚
     â”‚                â”‚                  â”‚  reload()      â”‚
     â”‚                â”‚                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚                â”‚                  â”‚                â”‚
```

### ìë™ ë°°í¬ êµ¬í˜„

```python
# mlops/deployer.py
import httpx
import asyncio
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class ModelDeployer:
    """ëª¨ë¸ ìë™ ë°°í¬ ê´€ë¦¬"""
    
    def __init__(
        self,
        model_registry: ModelRegistry,
        evaluator: ChampionChallengerEvaluator,
        inference_service_url: str = "http://localhost:8001",
        notification_webhook: Optional[str] = None
    ):
        self.registry = model_registry
        self.evaluator = evaluator
        self.inference_url = inference_service_url
        self.webhook = notification_webhook
    
    async def deploy_if_better(self, test_data: List[Dict]) -> Dict:
        """
        Challengerê°€ ë” ë‚˜ìœ¼ë©´ ìë™ ë°°í¬
        """
        # 1. Challenger ì¡´ì¬ í™•ì¸
        challenger = self.registry.get_challenger()
        if not challenger:
            return {"status": "no_challenger"}
        
        # 2. ë¹„êµ í‰ê°€
        comparison = self.evaluator.evaluate_on_test_set(test_data)
        logger.info(f"Comparison result: {comparison}")
        
        # 3. ìŠ¹ê²© íŒë‹¨
        if comparison["should_promote"]:
            return await self._promote_and_deploy(comparison)
        else:
            return await self._reject_challenger(comparison)
    
    async def _promote_and_deploy(self, comparison: Dict) -> Dict:
        """Challenger ìŠ¹ê²© ë° ë°°í¬"""
        try:
            # 1. Registryì—ì„œ ìŠ¹ê²©
            self.registry.promote_challenger()
            logger.info("Challenger promoted to Champion")
            
            # 2. Inference ì„œë¹„ìŠ¤ í•« ë¦¬ë¡œë“œ
            await self._reload_inference_service()
            logger.info("Inference service reloaded")
            
            # 3. ì•Œë¦¼ ë°œì†¡
            await self._send_notification({
                "event": "model_promoted",
                "improvement": comparison["improvement"],
                "new_champion_metrics": comparison["challenger"]
            })
            
            return {
                "status": "promoted",
                "comparison": comparison
            }
            
        except Exception as e:
            logger.error(f"Promotion failed: {e}")
            return {
                "status": "promotion_failed",
                "error": str(e)
            }
    
    async def _reject_challenger(self, comparison: Dict) -> Dict:
        """Challenger ê±°ë¶€"""
        self.registry.reject_challenger()
        
        await self._send_notification({
            "event": "challenger_rejected",
            "reason": "Performance not improved",
            "comparison": comparison
        })
        
        return {
            "status": "rejected",
            "comparison": comparison
        }
    
    async def _reload_inference_service(self):
        """Inference ì„œë¹„ìŠ¤ í•« ë¦¬ë¡œë“œ"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.inference_url}/reload",
                timeout=30.0
            )
            response.raise_for_status()
    
    async def _send_notification(self, payload: Dict):
        """ì•Œë¦¼ ë°œì†¡ (Slack/Discord ë“±)"""
        if not self.webhook:
            return
        
        async with httpx.AsyncClient() as client:
            await client.post(self.webhook, json=payload)


class DeploymentPipeline:
    """ì „ì²´ ë°°í¬ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"""
    
    def __init__(
        self,
        trainer: QLoRATrainer,
        data_preparator: DatasetPreparator,
        registry: ModelRegistry,
        evaluator: ChampionChallengerEvaluator,
        deployer: ModelDeployer
    ):
        self.trainer = trainer
        self.data_prep = data_preparator
        self.registry = registry
        self.evaluator = evaluator
        self.deployer = deployer
    
    async def run_full_pipeline(self) -> Dict:
        """
        ì „ì²´ MLOps íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        1. ë°ì´í„° ì¤€ë¹„
        2. í•™ìŠµ
        3. Challenger ë“±ë¡
        4. í‰ê°€ & ë°°í¬ íŒë‹¨
        """
        logger.info("Starting full MLOps pipeline")
        
        # 1. ë°ì´í„° ì¤€ë¹„
        dataset = self.data_prep.prepare_training_data()
        logger.info(f"Dataset prepared: {len(dataset['train'])} train, {len(dataset['test'])} test")
        
        # 2. í•™ìŠµ
        run_id = self.trainer.train(
            train_dataset=dataset["train"],
            eval_dataset=dataset["test"]
        )
        logger.info(f"Training completed: {run_id}")
        
        # 3. Challenger ë“±ë¡
        self.registry.register_challenger(run_id)
        logger.info("Challenger registered")
        
        # 4. í‰ê°€ & ë°°í¬
        result = await self.deployer.deploy_if_better(dataset["test"])
        logger.info(f"Deployment result: {result['status']}")
        
        return result
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ & ì•Œë¦¼

### Prometheus ë©”íŠ¸ë¦­ ì •ì˜

```python
# mlops/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info

# ëª¨ë¸ ì •ë³´
model_info = Info(
    "content_arena_model",
    "Current model information"
)

# ëª¨ë¸ ë²„ì „
model_version = Gauge(
    "content_arena_model_version",
    "Current model version",
    ["alias"]  # champion, challenger
)

# F1 Score
model_f1_score = Gauge(
    "content_arena_model_f1_score",
    "Model F1 score",
    ["alias"]
)

# ì¶”ë¡  ìš”ì²­
inference_requests = Counter(
    "content_arena_inference_requests_total",
    "Total inference requests",
    ["model_alias", "result"]  # result: toxic, clean
)

# ì¶”ë¡  ì§€ì—° ì‹œê°„
inference_latency = Histogram(
    "content_arena_inference_latency_seconds",
    "Inference latency in seconds",
    ["model_alias"],
    buckets=[0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0]
)

# ë°°í‹€ ê²°ê³¼
battle_detection_rate = Gauge(
    "content_arena_battle_detection_rate",
    "Current battle detection rate"
)

# ì¬í•™ìŠµ ì´ë²¤íŠ¸
retrain_events = Counter(
    "content_arena_retrain_events_total",
    "Total retrain events",
    ["trigger_reason", "result"]  # result: success, failed
)

# Champion êµì²´
champion_changes = Counter(
    "content_arena_champion_changes_total",
    "Total champion model changes"
)


class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def update_model_info(self, alias: str, version: int, f1: float):
        model_version.labels(alias=alias).set(version)
        model_f1_score.labels(alias=alias).set(f1)
    
    def record_inference(self, alias: str, is_toxic: bool, latency: float):
        result = "toxic" if is_toxic else "clean"
        inference_requests.labels(model_alias=alias, result=result).inc()
        inference_latency.labels(model_alias=alias).observe(latency)
    
    def update_detection_rate(self, rate: float):
        battle_detection_rate.set(rate)
    
    def record_retrain(self, reason: str, success: bool):
        result = "success" if success else "failed"
        retrain_events.labels(trigger_reason=reason, result=result).inc()
    
    def record_champion_change(self):
        champion_changes.inc()
```

### Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
// grafana/dashboards/content-arena.json
{
  "title": "Content Arena MLOps",
  "panels": [
    {
      "title": "Detection Rate Over Time",
      "type": "timeseries",
      "targets": [
        {
          "expr": "content_arena_battle_detection_rate",
          "legendFormat": "Detection Rate"
        }
      ]
    },
    {
      "title": "Model F1 Score",
      "type": "gauge",
      "targets": [
        {
          "expr": "content_arena_model_f1_score{alias=\"champion\"}",
          "legendFormat": "Champion F1"
        },
        {
          "expr": "content_arena_model_f1_score{alias=\"challenger\"}",
          "legendFormat": "Challenger F1"
        }
      ]
    },
    {
      "title": "Inference Latency (p99)",
      "type": "timeseries",
      "targets": [
        {
          "expr": "histogram_quantile(0.99, rate(content_arena_inference_latency_seconds_bucket[5m]))",
          "legendFormat": "p99 Latency"
        }
      ]
    },
    {
      "title": "Champion Changes",
      "type": "stat",
      "targets": [
        {
          "expr": "increase(content_arena_champion_changes_total[24h])",
          "legendFormat": "Changes (24h)"
        }
      ]
    },
    {
      "title": "Inference Requests by Result",
      "type": "piechart",
      "targets": [
        {
          "expr": "sum by (result) (increase(content_arena_inference_requests_total[1h]))",
          "legendFormat": "{{result}}"
        }
      ]
    }
  ]
}
```

### ì•Œë¦¼ ê·œì¹™

```yaml
# prometheus/alerts.yml
groups:
  - name: content-arena
    rules:
      # íƒì§€ìœ¨ ê¸‰ë½
      - alert: DetectionRateDrop
        expr: content_arena_battle_detection_rate < 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Detection rate dropped below 50%"
          description: "Current detection rate: {{ $value }}"
      
      # ëª¨ë¸ F1 ì €í•˜
      - alert: ModelF1ScoreDrop
        expr: content_arena_model_f1_score{alias="champion"} < 0.7
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Champion model F1 score dropped"
          description: "Current F1: {{ $value }}"
      
      # ì¶”ë¡  ì§€ì—°
      - alert: HighInferenceLatency
        expr: histogram_quantile(0.99, rate(content_arena_inference_latency_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High inference latency detected"
          description: "p99 latency: {{ $value }}s"
      
      # Challenger ì¥ê¸° ëŒ€ê¸°
      - alert: ChallengerStale
        expr: time() - content_arena_challenger_created_at > 86400
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "Challenger model waiting for over 24 hours"
```

---

## ğŸ”„ CI/CD í†µí•©

### GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/mlops.yml
name: MLOps Pipeline

on:
  push:
    paths:
      - 'ml-service/**'
      - 'training/**'
  schedule:
    # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì¬í•™ìŠµ (ì„ íƒì )
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retrain regardless of threshold'
        type: boolean
        default: false

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          cd ml-service
          pip install -r requirements-test.txt
      - name: Run tests
        run: |
          cd ml-service
          pytest tests/

  train:
    needs: test
    runs-on: [self-hosted, gpu]  # GPU ëŸ¬ë„ˆ í•„ìš”
    if: github.event_name == 'schedule' || github.event.inputs.force_retrain == 'true'
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          pip install -r ml-service/requirements.txt
      - name: Run training
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        run: |
          python training/train.py
      - name: Evaluate and deploy
        run: |
          python training/evaluate_and_deploy.py

  notify:
    needs: train
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify Slack
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          fields: repo,message,commit,author
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
mlops/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ model_registry.py      # MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬
â”œâ”€â”€ evaluator.py           # Champion/Challenger í‰ê°€
â”œâ”€â”€ deployer.py            # ìë™ ë°°í¬
â”œâ”€â”€ metrics.py             # Prometheus ë©”íŠ¸ë¦­
â”œâ”€â”€ alerting.py            # ì•Œë¦¼ ì‹œìŠ¤í…œ
â””â”€â”€ config.py              # MLOps ì„¤ì •

training/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ data_preparation.py    # ë°ì´í„°ì…‹ ì¤€ë¹„
â”œâ”€â”€ qlora_trainer.py       # QLoRA í•™ìŠµ
â”œâ”€â”€ auto_retrain.py        # ìë™ ì¬í•™ìŠµ íŠ¸ë¦¬ê±°
â”œâ”€â”€ train.py               # í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â””â”€â”€ evaluate_and_deploy.py # í‰ê°€ & ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
```
