# í•œêµ­ì–´ ë…ì„± í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ê°œì„  ë°©í–¥

> ì‘ì„±ì¼: 2026-01-23
> í˜„ì¬ ìµœê³  ì„±ëŠ¥: F1 0.9594 (ì•™ìƒë¸”)

## 1. í˜„ì¬ ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1.1 KcELECTRA-base-v2022 êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: í•œêµ­ì–´ í…ìŠ¤íŠ¸ (max 256 tokens)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Embedding Layer                                         â”‚
â”‚    â”œâ”€ Token Embedding (54,343 vocab)                    â”‚
â”‚    â”œâ”€ Position Embedding (512 max)                      â”‚
â”‚    â””â”€ Segment Embedding                                  â”‚
â”‚    â†’ Output: [batch, seq_len, 768]                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Transformer Encoder Ã— 12 Layers                         â”‚
â”‚    â”œâ”€ Multi-Head Self-Attention (12 heads, 64 dim each) â”‚
â”‚    â”‚   â””â”€ Attention(Q,K,V) = softmax(QK^T/âˆšd)V          â”‚
â”‚    â”œâ”€ Feed-Forward Network                               â”‚
â”‚    â”‚   â””â”€ FFN(x) = GELU(xWâ‚+bâ‚)Wâ‚‚+bâ‚‚                    â”‚
â”‚    â”‚   â””â”€ 768 â†’ 3072 â†’ 768                              â”‚
â”‚    â””â”€ Layer Norm + Residual Connection                  â”‚
â”‚    â†’ Output: [batch, seq_len, 768]                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Pooling: [CLS] token extraction                         â”‚
â”‚    â†’ Output: [batch, 768]                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Classification Head                                     â”‚
â”‚    â””â”€ Dropout(0.1) â†’ Linear(768â†’2) â†’ Softmax            â”‚
â”‚    â†’ Output: [batch, 2] (ì •ìƒ/ë…ì„± í™•ë¥ )                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì´ íŒŒë¼ë¯¸í„°: ~110M
í•™ìŠµ ê°€ëŠ¥: ì „ì²´ (Fine-tuning)
```

### 1.2 í˜„ì¬ ì•™ìƒë¸” êµ¬ì„±

```
Phase 2 Model (KcELECTRA)     Phase 4 Model (KcELECTRA)
        â”‚ weight=0.6                  â”‚ weight=0.4
        â–¼                             â–¼
   [prob_clean, prob_toxic]    [prob_clean, prob_toxic]
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         Weighted Average
                   â”‚
                   â–¼
         threshold > 0.5 â†’ Toxic
```

## 2. í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ

| ëª¨ë¸ | F1 | Precision | Recall | FP | FN |
|------|-----|-----------|--------|-----|-----|
| Phase 2 (ë‹¨ë…) | 0.9597 | - | - | 80 | 164 |
| Phase 4 (ë‹¨ë…) | 0.9580 | - | - | 98 | 137 |
| **ì•™ìƒë¸” (0.6:0.4)** | **0.9594** | - | - | **78** | **150** |

### 2.1 ì—ëŸ¬ ë¶„ì„

**False Negative ì£¼ìš” íŒ¨í„´:**
1. ë§¥ë½ ì˜ì¡´ì  í‘œí˜„: "ë°±ë¦°íƒ„ì´ í•„ìš”í•˜ë‹¤", "ì•ì°¨ ìµœì†Œ ì „ë¼ë„"
2. ì•”ì‹œì  í˜ì˜¤: "ì—¬íŒì‚¬ë„¤", "ë•…í¬ ë¶€ë¦‰ë¶€ë¦‰"
3. ë‚œë…í™” ë³€ì¢…: ã……ã…‚, ì‹œã…‚, ì”¨ã„¹ ë“±

**False Positive ì£¼ìš” íŒ¨í„´:**
1. ë¬´ê¸°/í­ë ¥ ë‹¨ì–´ì˜ ì •ìƒ ë¬¸ë§¥ ì‚¬ìš©
2. ìœ ì‚¬ ìš•ì„¤ íŒ¨í„´ ì˜¤íƒ

## 3. ìµœì‹  ì—°êµ¬ ë™í–¥ (2025)

### 3.1 PMF (Parallel Model Fusion) - Nature Scientific Reports

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BERT   â”‚  â”‚DistilBERTâ”‚  â”‚ RoBERTa  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Meta-Learner â”‚
           â”‚  (RF, SVM)    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **ì„±ëŠ¥**: í•œêµ­ì–´ 89% accuracy, ì˜ì–´ 85%
- **íŠ¹ì§•**: Thompson Samplingìœ¼ë¡œ ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •
- **ì°¸ê³ **: https://www.nature.com/articles/s41598-025-88960-y

### 3.2 CNN + Transformer ê²°í•©

```
Transformer Output [batch, seq_len, 768]
        â”‚
        â–¼
   Conv1D Layers (ë‹¤ì–‘í•œ kernel size)
   â”œâ”€ kernel=2 (bigram íŒ¨í„´)
   â”œâ”€ kernel=3 (trigram íŒ¨í„´)
   â””â”€ kernel=4 (4-gram íŒ¨í„´)
        â”‚
        â–¼
   MaxPooling + Concatenate
        â”‚
        â–¼
   Classification Head
```

- **ì¥ì **: CNNì´ ë¡œì»¬ n-gram íŒ¨í„´(ìš•ì„¤) í¬ì°©, Transformerê°€ ì „ì—­ ë§¥ë½ ì´í•´
- **ì°¸ê³ **: https://arxiv.org/html/2511.06051v1

### 3.3 LoRA ê¸°ë°˜ ê²½ëŸ‰í™”

- ì „ì²´ ëª¨ë¸ ëŒ€ì‹  3ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ì°¸ê³ : https://arxiv.org/html/2511.06051v1

## 4. ê°œì„  ë°©í–¥

### 4.1 Phase 5: CNN-Enhanced Model (ê¶Œì¥)

```python
class CNNEnhancedClassifier(nn.Module):
    """
    Transformer + CNN ê²°í•© ëª¨ë¸

    ì¥ì :
    - CNNì´ ìš•ì„¤ n-gram íŒ¨í„´ ì§ì ‘ í¬ì°©
    - Transformerê°€ ë§¥ë½ ì´í•´
    - ë‘ ì •ë³´ ê²°í•©ìœ¼ë¡œ FN/FP ë™ì‹œ ê°ì†Œ ê¸°ëŒ€
    """
    def __init__(self, base_model, hidden_size=768):
        self.transformer = base_model

        # Multi-scale CNN for n-gram patterns
        self.conv_layers = nn.ModuleList([
            nn.Conv1d(hidden_size, 128, kernel_size=2),  # bigram
            nn.Conv1d(hidden_size, 128, kernel_size=3),  # trigram
            nn.Conv1d(hidden_size, 128, kernel_size=4),  # 4-gram
        ])

        # Combined classifier
        # 768 (CLS) + 128*3 (CNN) = 1152
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(768 + 128*3, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 2)
        )
```

**ì˜ˆìƒ íš¨ê³¼:**
- F1: 0.9594 â†’ 0.965+ (+0.5~1%)
- FN ê°ì†Œ: ë‚œë…í™” ìš•ì„¤ íŒ¨í„´ ì§ì ‘ í¬ì°©
- FP ê°ì†Œ: ë§¥ë½ ì •ë³´ì™€ ê²°í•©ìœ¼ë¡œ ì˜¤íƒ ê°ì†Œ

### 4.2 Phase 6: Meta-Learner Ensemble

```python
# Phase 2, 4, 5 ëª¨ë¸ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ
from sklearn.ensemble import RandomForestClassifier

meta_features = np.column_stack([
    phase2_probs,  # [N, 2]
    phase4_probs,  # [N, 2]
    phase5_probs,  # [N, 2]
])

meta_learner = RandomForestClassifier(n_estimators=100)
meta_learner.fit(meta_features, labels)
```

### 4.3 Phase 7: ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ

```
Input â†’ Transformer â†’ Shared Representation
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼              â–¼              â–¼
      Binary Task    Type Task     Severity Task
      (ë…ì„± ì—¬ë¶€)    (í˜ì˜¤ ìœ í˜•)    (ì‹¬ê°ë„ 1-5)
```

## 5. ìš°ì„ ìˆœìœ„ ë° ì¼ì •

| ë‹¨ê³„ | ì‘ì—… | ì˜ˆìƒ íš¨ê³¼ | ë‚œì´ë„ | ìš°ì„ ìˆœìœ„ |
|------|------|----------|--------|---------|
| Phase 5 | CNN ë ˆì´ì–´ ì¶”ê°€ | F1 +0.5~1% | ì¤‘ê°„ | ğŸ”´ ë†’ìŒ |
| Phase 6 | Meta-Learner | F1 +0.3~0.5% | ë‚®ìŒ | ğŸŸ¡ ì¤‘ê°„ |
| Phase 7 | ë©€í‹°íƒœìŠ¤í¬ | ì„¸ë°€í•œ ë¶„ë¥˜ | ë†’ìŒ | ğŸŸ¢ ë‚®ìŒ |
| - | ì •ìƒ ë°ì´í„° ì¦ê°• | FP -20~30% | ë‚®ìŒ | ğŸ”´ ë†’ìŒ |

## 6. ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- [Adaptive ensemble techniques (Nature 2025)](https://www.nature.com/articles/s41598-025-88960-y)
- [Korean Political Hate Speech (Springer 2024)](https://link.springer.com/article/10.1007/s10579-024-09797-x)
- [3-Layer LoRA BERTweet (arXiv 2025)](https://arxiv.org/html/2511.06051v1)
- [K-HATERS Corpus (EMNLP 2023)](https://aclanthology.org/2023.findings-emnlp.952.pdf)

### ë°ì´í„°ì…‹
- [BEEP! Korean Toxic Speech](https://github.com/kocohub/korean-hate-speech)
- [K-MHaS Multi-label Hate Speech](https://github.com/adlnlp/K-MHaS)
- [Korean Hate Speech (HuggingFace)](https://huggingface.co/datasets/nayohan/korean-hate-speech)

### ëª¨ë¸
- [KcELECTRA](https://github.com/Beomi/KcELECTRA)
- [KoELECTRA](https://github.com/monologg/KoELECTRA)
